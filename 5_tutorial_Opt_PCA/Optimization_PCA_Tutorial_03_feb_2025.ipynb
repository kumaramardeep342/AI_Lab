{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Optimization with SciPy and Dimensionality Reduction with PCA\n",
    "## DA5401W - Foundations of Machine Learning\n",
    "### Instructor: Dr. Arun B Ayyar\n",
    "### IIT Madras\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This tutorial contains **hands-on problems** for:\n",
    "1. **Optimization using SciPy** (Linear Programming, Nonlinear Optimization, Constrained Optimization)\n",
    "2. **Dimensionality Reduction using PCA** (Feature extraction, visualization, reconstruction)\n",
    "\n",
    "**How to use this notebook:**\n",
    "- Each problem has a **Problem Statement** with data setup\n",
    "- Try solving the problem yourself first\n",
    "- Solutions are hidden - click **\"Show Solution\"** to reveal\n",
    "- Run all cells to load the data and helper functions\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Apply scipy.optimize for real-world optimization problems\n",
    "- Understand when to use different optimization methods\n",
    "- Implement PCA for dimensionality reduction\n",
    "- Interpret principal components and explained variance\n",
    "- Apply PCA to real datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize, linprog, LinearConstraint, NonlinearConstraint\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer, load_wine, fetch_olivetti_faces\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(\"‚úì Ready to start the tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Optimization with SciPy\n",
    "\n",
    "## Problem 1: Production Planning (Linear Programming)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "A factory produces two products: **Product A** and **Product B**.\n",
    "\n",
    "**Profit:**\n",
    "- Product A: ‚Çπ50 per unit\n",
    "- Product B: ‚Çπ40 per unit\n",
    "\n",
    "**Resource Constraints:**\n",
    "- **Labor hours**: Product A needs 2 hours, Product B needs 1 hour. Total available: 100 hours\n",
    "- **Raw material**: Product A needs 1 kg, Product B needs 2 kg. Total available: 80 kg\n",
    "- **Machine time**: Product A needs 1 hour, Product B needs 1 hour. Total available: 60 hours\n",
    "\n",
    "**Question:** How many units of each product should be produced to maximize profit?\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Maximize: $Z = 50x_1 + 40x_2$\n",
    "\n",
    "Subject to:\n",
    "- $2x_1 + x_2 \\leq 100$ (labor)\n",
    "- $x_1 + 2x_2 \\leq 80$ (raw material)\n",
    "- $x_1 + x_2 \\leq 60$ (machine time)\n",
    "- $x_1, x_2 \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Problem 1\n",
    "# Objective function coefficients (we need to minimize, so negate for maximization)\n",
    "c = [-50, -40]  # Negative because linprog minimizes\n",
    "\n",
    "# Inequality constraint matrix (A_ub @ x <= b_ub)\n",
    "A_ub = np.array([\n",
    "    [2, 1],   # Labor constraint\n",
    "    [1, 2],   # Raw material constraint\n",
    "    [1, 1]    # Machine time constraint\n",
    "])\n",
    "\n",
    "b_ub = np.array([100, 80, 60])\n",
    "\n",
    "# Bounds for variables (x1 >= 0, x2 >= 0)\n",
    "bounds = [(0, None), (0, None)]\n",
    "\n",
    "print(\"Problem 1 Data Loaded\")\n",
    "print(\"=\"*60)\n",
    "print(\"Objective: Maximize 50*x1 + 40*x2\")\n",
    "print(\"\\nConstraints:\")\n",
    "print(\"  2*x1 + 1*x2 <= 100  (Labor)\")\n",
    "print(\"  1*x1 + 2*x2 <= 80   (Raw material)\")\n",
    "print(\"  1*x1 + 1*x2 <= 60   (Machine time)\")\n",
    "print(\"  x1, x2 >= 0\")\n",
    "print(\"\\n‚Üí YOUR TASK: Use scipy.optimize.linprog to solve this problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "# Use: result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üìñ Show Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Solution for Problem 1\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Solve the linear program\n",
    "result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
    "\n",
    "print(\"Solution for Problem 1: Production Planning\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Status: {result.message}\")\n",
    "print(f\"\\nOptimal Production:\")\n",
    "print(f\"  Product A: {result.x[0]:.2f} units\")\n",
    "print(f\"  Product B: {result.x[1]:.2f} units\")\n",
    "print(f\"\\nMaximum Profit: ‚Çπ{-result.fun:.2f}\")  # Negative because we minimized\n",
    "\n",
    "# Check resource utilization\n",
    "print(f\"\\nResource Utilization:\")\n",
    "labor_used = 2*result.x[0] + result.x[1]\n",
    "material_used = result.x[0] + 2*result.x[1]\n",
    "machine_used = result.x[0] + result.x[1]\n",
    "print(f\"  Labor: {labor_used:.2f} / 100 hours ({labor_used/100*100:.1f}%)\")\n",
    "print(f\"  Raw Material: {material_used:.2f} / 80 kg ({material_used/80*100:.1f}%)\")\n",
    "print(f\"  Machine Time: {machine_used:.2f} / 60 hours ({machine_used/60*100:.1f}%)\")\n",
    "\n",
    "# Visualize the feasible region\n",
    "x1 = np.linspace(0, 60, 400)\n",
    "x2_labor = 100 - 2*x1\n",
    "x2_material = (80 - x1) / 2\n",
    "x2_machine = 60 - x1\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(x1, x2_labor, 'r-', label='Labor: 2x‚ÇÅ + x‚ÇÇ ‚â§ 100', linewidth=2)\n",
    "plt.plot(x1, x2_material, 'b-', label='Material: x‚ÇÅ + 2x‚ÇÇ ‚â§ 80', linewidth=2)\n",
    "plt.plot(x1, x2_machine, 'g-', label='Machine: x‚ÇÅ + x‚ÇÇ ‚â§ 60', linewidth=2)\n",
    "\n",
    "# Fill feasible region\n",
    "x1_fill = np.linspace(0, 60, 400)\n",
    "x2_upper = np.minimum(np.minimum(100 - 2*x1_fill, (80 - x1_fill)/2), 60 - x1_fill)\n",
    "x2_upper = np.maximum(x2_upper, 0)\n",
    "plt.fill_between(x1_fill, 0, x2_upper, alpha=0.2, color='yellow', label='Feasible Region')\n",
    "\n",
    "# Plot optimal point\n",
    "plt.plot(result.x[0], result.x[1], 'r*', markersize=20, label=f'Optimal: ({result.x[0]:.1f}, {result.x[1]:.1f})')\n",
    "\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 60)\n",
    "plt.xlabel('Product A (x‚ÇÅ)', fontsize=12)\n",
    "plt.ylabel('Product B (x‚ÇÇ)', fontsize=12)\n",
    "plt.title('Linear Programming: Production Planning', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Expected Answer:**\n",
    "- Product A: 40 units\n",
    "- Product B: 20 units\n",
    "- Maximum Profit: ‚Çπ2800\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Portfolio Optimization (Quadratic Programming)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "You want to invest in 3 stocks. Historical data shows:\n",
    "\n",
    "**Expected Returns:**\n",
    "- Stock 1: 12% per year\n",
    "- Stock 2: 10% per year\n",
    "- Stock 3: 8% per year\n",
    "\n",
    "**Covariance Matrix** (risk):\n",
    "```\n",
    "        Stock1  Stock2  Stock3\n",
    "Stock1   0.04    0.01    0.00\n",
    "Stock2   0.01    0.03    0.01\n",
    "Stock3   0.00    0.01    0.02\n",
    "```\n",
    "\n",
    "**Question:** Find the portfolio weights that **minimize risk** (variance) while achieving **at least 10% expected return**.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Minimize: $\\frac{1}{2}w^T \\Sigma w$ (portfolio variance)\n",
    "\n",
    "Subject to:\n",
    "- $\\mu^T w \\geq 0.10$ (minimum return)\n",
    "- $\\sum w_i = 1$ (fully invested)\n",
    "- $w_i \\geq 0$ (no short selling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Problem 2\n",
    "# Expected returns\n",
    "mu = np.array([0.12, 0.10, 0.08])\n",
    "\n",
    "# Covariance matrix\n",
    "Sigma = np.array([\n",
    "    [0.04, 0.01, 0.00],\n",
    "    [0.01, 0.03, 0.01],\n",
    "    [0.00, 0.01, 0.02]\n",
    "])\n",
    "\n",
    "print(\"Problem 2 Data Loaded\")\n",
    "print(\"=\"*60)\n",
    "print(\"Expected Returns:\", mu)\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(Sigma)\n",
    "print(\"\\n‚Üí YOUR TASK: Find optimal portfolio weights\")\n",
    "print(\"   - Minimize portfolio variance\")\n",
    "print(\"   - Expected return >= 10%\")\n",
    "print(\"   - Weights sum to 1\")\n",
    "print(\"   - No short selling (w >= 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "# Hint: Define objective function as: lambda w: 0.5 * w @ Sigma @ w\n",
    "# Use LinearConstraint and NonlinearConstraint for constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üìñ Show Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Solution for Problem 2\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "\n",
    "# Objective: minimize portfolio variance\n",
    "def portfolio_variance(w):\n",
    "    return 0.5 * w.T @ Sigma @ w\n",
    "\n",
    "# Gradient of objective\n",
    "def portfolio_variance_grad(w):\n",
    "    return Sigma @ w\n",
    "\n",
    "# Constraints\n",
    "# 1. Sum of weights = 1\n",
    "constraint_sum = LinearConstraint(np.ones(3), 1, 1)\n",
    "\n",
    "# 2. Expected return >= 0.10\n",
    "constraint_return = LinearConstraint(mu, 0.10, np.inf)\n",
    "\n",
    "# Bounds: no short selling\n",
    "bounds = [(0, 1) for _ in range(3)]\n",
    "\n",
    "# Initial guess\n",
    "w0 = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Solve\n",
    "result = minimize(portfolio_variance, w0, method='SLSQP', jac=portfolio_variance_grad,\n",
    "                  constraints=[constraint_sum, constraint_return], bounds=bounds)\n",
    "\n",
    "print(\"Solution for Problem 2: Portfolio Optimization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Status: {result.message}\")\n",
    "print(f\"\\nOptimal Weights:\")\n",
    "for i, w in enumerate(result.x, 1):\n",
    "    print(f\"  Stock {i}: {w*100:.2f}%\")\n",
    "\n",
    "expected_return = mu @ result.x\n",
    "portfolio_risk = np.sqrt(result.x @ Sigma @ result.x)\n",
    "\n",
    "print(f\"\\nPortfolio Metrics:\")\n",
    "print(f\"  Expected Return: {expected_return*100:.2f}%\")\n",
    "print(f\"  Portfolio Risk (Std Dev): {portfolio_risk*100:.2f}%\")\n",
    "print(f\"  Sharpe Ratio (assuming rf=2%): {(expected_return-0.02)/portfolio_risk:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart of weights\n",
    "axes[0].pie(result.x, labels=[f'Stock {i+1}' for i in range(3)], autopct='%1.1f%%',\n",
    "            colors=['#ff9999','#66b3ff','#99ff99'], startangle=90)\n",
    "axes[0].set_title('Optimal Portfolio Allocation', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "stocks = ['Stock 1', 'Stock 2', 'Stock 3']\n",
    "x_pos = np.arange(len(stocks))\n",
    "axes[1].bar(x_pos, result.x*100, color=['#ff9999','#66b3ff','#99ff99'], edgecolor='black')\n",
    "axes[1].set_xlabel('Stock', fontsize=12)\n",
    "axes[1].set_ylabel('Weight (%)', fontsize=12)\n",
    "axes[1].set_title('Portfolio Weights', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(stocks)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Expected Answer:**\n",
    "- Approximately: Stock 1: 50%, Stock 2: 50%, Stock 3: 0%\n",
    "- Expected Return: 11%\n",
    "- Portfolio Risk: ~17%\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Rosenbrock Function Minimization\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The **Rosenbrock function** is a famous test function for optimization algorithms:\n",
    "\n",
    "$$f(x, y) = (1-x)^2 + 100(y-x^2)^2$$\n",
    "\n",
    "It has a narrow parabolic valley with the global minimum at $(1, 1)$ where $f(1, 1) = 0$.\n",
    "\n",
    "**Question:** \n",
    "1. Minimize the Rosenbrock function starting from $(-1.5, 2.5)$\n",
    "2. Compare different optimization methods: `'BFGS'`, `'Nelder-Mead'`, `'CG'`\n",
    "3. Which method converges fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Problem 3\n",
    "def rosenbrock(x):\n",
    "    \"\"\"Rosenbrock function\"\"\"\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_grad(x):\n",
    "    \"\"\"Gradient of Rosenbrock function\"\"\"\n",
    "    dfdx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    dfdy = 200*(x[1] - x[0]**2)\n",
    "    return np.array([dfdx, dfdy])\n",
    "\n",
    "# Starting point\n",
    "x0 = np.array([-1.5, 2.5])\n",
    "\n",
    "print(\"Problem 3 Data Loaded\")\n",
    "print(\"=\"*60)\n",
    "print(\"Function: f(x,y) = (1-x)¬≤ + 100(y-x¬≤)¬≤\")\n",
    "print(f\"Starting point: {x0}\")\n",
    "print(f\"Function value at start: {rosenbrock(x0):.2f}\")\n",
    "print(\"\\n‚Üí YOUR TASK: Minimize using different methods and compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "# Try methods: 'BFGS', 'Nelder-Mead', 'CG'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üìñ Show Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Solution for Problem 3\n",
    "methods = ['BFGS', 'Nelder-Mead', 'CG']\n",
    "results = {}\n",
    "\n",
    "print(\"Solution for Problem 3: Rosenbrock Function\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method in methods:\n",
    "    if method in ['BFGS', 'CG']:\n",
    "        result = minimize(rosenbrock, x0, method=method, jac=rosenbrock_grad)\n",
    "    else:\n",
    "        result = minimize(rosenbrock, x0, method=method)\n",
    "    \n",
    "    results[method] = result\n",
    "    \n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    print(f\"  Solution: x = {result.x[0]:.6f}, y = {result.x[1]:.6f}\")\n",
    "    print(f\"  Function value: {result.fun:.10f}\")\n",
    "    print(f\"  Iterations: {result.nit}\")\n",
    "    print(f\"  Function evaluations: {result.nfev}\")\n",
    "    print(f\"  Success: {result.success}\")\n",
    "\n",
    "# Visualize the function and optimization paths\n",
    "x = np.linspace(-2, 2, 400)\n",
    "y = np.linspace(-1, 3, 400)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (1 - X)**2 + 100*(Y - X**2)**2\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Contour plot\n",
    "levels = np.logspace(-1, 3.5, 20)\n",
    "contour = plt.contour(X, Y, Z, levels=levels, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(contour, label='f(x, y)')\n",
    "\n",
    "# Plot starting point\n",
    "plt.plot(x0[0], x0[1], 'ko', markersize=12, label='Start', zorder=5)\n",
    "\n",
    "# Plot optimal point\n",
    "plt.plot(1, 1, 'r*', markersize=20, label='Global Minimum (1,1)', zorder=5)\n",
    "\n",
    "# Plot solutions\n",
    "colors = ['blue', 'green', 'orange']\n",
    "for (method, result), color in zip(results.items(), colors):\n",
    "    plt.plot(result.x[0], result.x[1], 'o', color=color, markersize=10, \n",
    "             label=f'{method}: ({result.x[0]:.3f}, {result.x[1]:.3f})', zorder=5)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Rosenbrock Function: Optimization Methods Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparison table\n",
    "comparison_data = []\n",
    "for method, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Method': method,\n",
    "        'Final x': f'{result.x[0]:.6f}',\n",
    "        'Final y': f'{result.x[1]:.6f}',\n",
    "        'f(x,y)': f'{result.fun:.2e}',\n",
    "        'Iterations': result.nit,\n",
    "        'Func Evals': result.nfev\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison Summary:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "```\n",
    "\n",
    "**Expected Answer:**\n",
    "- All methods should converge to approximately (1.0, 1.0)\n",
    "- BFGS typically converges fastest (fewest iterations)\n",
    "- Nelder-Mead requires more function evaluations\n",
    "- CG is in between\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Breast Cancer Dataset - Feature Extraction\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The **Breast Cancer Wisconsin dataset** contains 569 samples of breast cancer tumors with 30 features computed from digitized images:\n",
    "- Features include: radius, texture, perimeter, area, smoothness, compactness, concavity, etc.\n",
    "- Each feature is computed as mean, standard error, and \"worst\" (mean of 3 largest values)\n",
    "- Examples: mean radius, radius error, worst radius\n",
    "\n",
    "There are 2 classes: **Malignant** (cancerous) and **Benign** (non-cancerous).\n",
    "\n",
    "**Questions:**\n",
    "1. Apply PCA to reduce from 30D to 2D\n",
    "2. How much variance is explained by the first 2 principal components?\n",
    "3. Visualize the data in the 2D PCA space\n",
    "4. Which original features contribute most to PC1 and PC2?\n",
    "5. Are the two classes (malignant vs benign) separable in 2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Problem 4\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "feature_names_cancer = cancer.feature_names\n",
    "target_names_cancer = cancer.target_names\n",
    "\n",
    "print(\"Problem 4 Data Loaded: Breast Cancer Dataset\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of samples: {X_cancer.shape[0]}\")\n",
    "print(f\"Number of features: {X_cancer.shape[1]}\")\n",
    "print(f\"\\nFirst 5 feature names:\")\n",
    "for i, name in enumerate(feature_names_cancer[:5]):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "print(f\"  ... and {len(feature_names_cancer)-5} more features\")\n",
    "print(f\"\\nTarget names: {target_names_cancer}\")\n",
    "print(f\"  - Malignant: {np.sum(y_cancer == 0)} samples\")\n",
    "print(f\"  - Benign: {np.sum(y_cancer == 1)} samples\")\n",
    "print(f\"\\nFirst 5 samples (first 5 features):\")\n",
    "print(pd.DataFrame(X_cancer[:5, :5], columns=feature_names_cancer[:5]))\n",
    "print(\"\\n‚Üí YOUR TASK: Apply PCA and answer the questions above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "# Steps:\n",
    "# 1. Standardize the data using StandardScaler\n",
    "# 2. Apply PCA with n_components=2\n",
    "# 3. Transform the data\n",
    "# 4. Analyze explained variance\n",
    "# 5. Visualize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üìñ Show Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Solution for Problem 4\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Standardize\n",
    "scaler = StandardScaler()\n",
    "X_cancer_scaled = scaler.fit_transform(X_cancer)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_cancer_pca = pca.fit_transform(X_cancer_scaled)\n",
    "\n",
    "print(\"Solution for Problem 4: Breast Cancer Dataset PCA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Question 2: Explained variance\n",
    "print(f\"\\nExplained Variance Ratio:\")\n",
    "print(f\"  PC1: {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"  PC2: {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(f\"  Total: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# Question 4: Component loadings (top 5 for each PC)\n",
    "print(f\"\\nTop 5 Features Contributing to Each PC:\")\n",
    "pc1_top_idx = np.argsort(np.abs(pca.components_[0]))[-5:][::-1]\n",
    "pc2_top_idx = np.argsort(np.abs(pca.components_[1]))[-5:][::-1]\n",
    "\n",
    "print(f\"\\nPC1:\")\n",
    "for idx in pc1_top_idx:\n",
    "    print(f\"  {feature_names_cancer[idx]}: {pca.components_[0, idx]:.3f}\")\n",
    "\n",
    "print(f\"\\nPC2:\")\n",
    "for idx in pc2_top_idx:\n",
    "    print(f\"  {feature_names_cancer[idx]}: {pca.components_[1, idx]:.3f}\")\n",
    "\n",
    "# Question 3 & 5: Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot in PCA space\n",
    "colors = ['red', 'blue']\n",
    "for i, (color, target_name) in enumerate(zip(colors, target_names_cancer)):\n",
    "    mask = y_cancer == i\n",
    "    axes[0].scatter(X_cancer_pca[mask, 0], X_cancer_pca[mask, 1], \n",
    "                    color=color, label=target_name, alpha=0.6, edgecolor='black', s=60)\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_title('Breast Cancer Dataset in PCA Space', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Biplot (showing top features only for clarity)\n",
    "for i, (color, target_name) in enumerate(zip(colors, target_names_cancer)):\n",
    "    mask = y_cancer == i\n",
    "    axes[1].scatter(X_cancer_pca[mask, 0], X_cancer_pca[mask, 1], \n",
    "                    color=color, label=target_name, alpha=0.3, s=40)\n",
    "\n",
    "# Add loading vectors for top 5 features\n",
    "scale = 8\n",
    "top_features_idx = list(set(pc1_top_idx[:3].tolist() + pc2_top_idx[:3].tolist()))\n",
    "for idx in top_features_idx:\n",
    "    axes[1].arrow(0, 0, pca.components_[0, idx]*scale, pca.components_[1, idx]*scale,\n",
    "                  head_width=0.3, head_length=0.3, fc='darkgreen', ec='darkgreen', linewidth=2)\n",
    "    # Shorten feature names\n",
    "    short_name = feature_names_cancer[idx].replace('mean ', '').replace('worst ', 'w_')\n",
    "    axes[1].text(pca.components_[0, idx]*scale*1.15, pca.components_[1, idx]*scale*1.15,\n",
    "                 short_name, fontsize=9, ha='center', \n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_title('PCA Biplot: Data + Top Feature Loadings', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Answer to Question 5\n",
    "print(\"\\nQuestion 5: Are malignant and benign tumors separable in 2D?\")\n",
    "print(\"  ‚Üí Yes! There is good separation between the two classes.\")\n",
    "print(\"  ‚Üí Malignant tumors tend to have higher PC1 values.\")\n",
    "print(\"  ‚Üí Some overlap exists, but the classes are largely distinguishable.\")\n",
    "print(\"  ‚Üí This suggests PCA captures important discriminative features.\")\n",
    "```\n",
    "\n",
    "**Expected Answer:**\n",
    "- PC1 explains ~44% variance, PC2 explains ~19%\n",
    "- Total: ~63% variance retained\n",
    "- PC1 is dominated by features like mean radius, mean perimeter, mean area, worst radius\n",
    "- PC2 is influenced by texture and smoothness features\n",
    "- Malignant and benign tumors show good separation with some overlap\n",
    "- Larger tumors (higher PC1) tend to be malignant\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: Olivetti Faces - Image Reconstruction\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The **Olivetti Faces dataset** contains 400 grayscale face images of 40 different people (10 images per person). Each image is 64√ó64 pixels = 4096 features.\n",
    "\n",
    "**Questions:**\n",
    "1. Apply PCA and create a **scree plot** to determine how many components to keep\n",
    "2. How many components are needed to retain 90% of variance?\n",
    "3. Reduce to that many components and reconstruct the images\n",
    "4. Visualize original vs reconstructed images\n",
    "5. What is the compression ratio achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Problem 5\n",
    "faces = fetch_olivetti_faces()\n",
    "X_faces = faces.data\n",
    "y_faces = faces.target\n",
    "\n",
    "print(\"Problem 5 Data Loaded: Olivetti Faces\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of samples: {X_faces.shape[0]}\")\n",
    "print(f\"Number of features (pixels): {X_faces.shape[1]}\")\n",
    "print(f\"Image shape: 64x64\")\n",
    "print(f\"Number of people: {len(np.unique(y_faces))}\")\n",
    "\n",
    "# Show some example images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_faces[i].reshape(64, 64), cmap='gray')\n",
    "    ax.set_title(f'Person {y_faces[i]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Face Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚Üí YOUR TASK: Apply PCA for image compression and reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "# Steps:\n",
    "# 1. Standardize the data\n",
    "# 2. Apply PCA with all components first\n",
    "# 3. Create scree plot\n",
    "# 4. Find number of components for 90% variance\n",
    "# 5. Reconstruct images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üìñ Show Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Solution for Problem 5\n",
    "\n",
    "# Step 1: Data is already normalized (0-1 range for images)\n",
    "# For faces, we can work with the data directly or standardize\n",
    "# Let's standardize for better PCA performance\n",
    "scaler_faces = StandardScaler()\n",
    "X_faces_scaled = scaler_faces.fit_transform(X_faces)\n",
    "\n",
    "# Step 2: Apply PCA with all components\n",
    "pca_full = PCA()\n",
    "X_faces_pca_full = pca_full.fit_transform(X_faces_scaled)\n",
    "\n",
    "print(\"Solution for Problem 5: Olivetti Faces PCA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Question 2: Components for 90% variance\n",
    "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_90 = np.argmax(cumsum_var >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nComponents needed for 90% variance: {n_components_90}\")\n",
    "print(f\"Actual variance retained: {cumsum_var[n_components_90-1]*100:.2f}%\")\n",
    "\n",
    "# Question 5: Compression ratio\n",
    "original_size = X_faces.shape[1]  # 4096 features\n",
    "compressed_size = n_components_90\n",
    "compression_ratio = original_size / compressed_size\n",
    "\n",
    "print(f\"\\nCompression:\")\n",
    "print(f\"  Original dimensions: {original_size}\")\n",
    "print(f\"  Compressed dimensions: {compressed_size}\")\n",
    "print(f\"  Compression ratio: {compression_ratio:.2f}x\")\n",
    "\n",
    "# Step 3: Scree plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, min(51, len(pca_full.explained_variance_ratio_)+1)), \n",
    "            pca_full.explained_variance_ratio_[:50], color='steelblue', edgecolor='black')\n",
    "axes[0].axhline(y=0.02, color='r', linestyle='--', label='2% threshold')\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "axes[0].set_title('Scree Plot: Individual Variance (First 50 PCs)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumsum_var)+1), cumsum_var*100, 'b-', linewidth=2)\n",
    "axes[1].axhline(y=90, color='r', linestyle='--', linewidth=2, label='90% threshold')\n",
    "axes[1].axvline(x=n_components_90, color='g', linestyle='--', linewidth=2, \n",
    "                label=f'{n_components_90} components')\n",
    "axes[1].scatter([n_components_90], [cumsum_var[n_components_90-1]*100], \n",
    "                color='red', s=100, zorder=5)\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Explained Variance (%)', fontsize=12)\n",
    "axes[1].set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, min(200, len(cumsum_var)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 4 & 5: Reconstruct images\n",
    "pca_compressed = PCA(n_components=n_components_90)\n",
    "X_faces_compressed = pca_compressed.fit_transform(X_faces_scaled)\n",
    "X_faces_reconstructed = pca_compressed.inverse_transform(X_faces_compressed)\n",
    "X_faces_reconstructed = scaler_faces.inverse_transform(X_faces_reconstructed)\n",
    "\n",
    "# Visualize original vs reconstructed\n",
    "n_samples = 5\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Original\n",
    "    axes[0, i].imshow(X_faces[i].reshape(64, 64), cmap='gray')\n",
    "    axes[0, i].set_title(f'Original\\nPerson {y_faces[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(X_faces_reconstructed[i].reshape(64, 64), cmap='gray')\n",
    "    axes[1, i].set_title(f'Reconstructed\\n({n_components_90} PCs)')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Face Reconstruction with PCA ({n_components_90} components, {compression_ratio:.1f}x compression)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize eigenfaces (principal components as images)\n",
    "print(\"\\nBonus: Visualizing Eigenfaces (First 10 Principal Components)\")\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    eigenface = pca_compressed.components_[i].reshape(64, 64)\n",
    "    ax.imshow(eigenface, cmap='gray')\n",
    "    ax.set_title(f'PC{i+1}\\n({pca_full.explained_variance_ratio_[i]*100:.1f}%)')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Eigenfaces: Principal Components as Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = np.mean((X_faces - X_faces_reconstructed)**2)\n",
    "print(f\"\\nReconstruction Error (MSE): {mse:.6f}\")\n",
    "```\n",
    "\n",
    "**Expected Answer:**\n",
    "- Need approximately 100-150 components for 90% variance\n",
    "- Compression ratio: ~27-40x (much better than digits!)\n",
    "- Reconstructed faces are very similar to originals\n",
    "- Eigenfaces show the most important facial features\n",
    "- First few eigenfaces capture lighting, face shape, and key facial features\n",
    "- Higher-order eigenfaces capture finer details\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6: Wine Dataset - Feature Importance\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The **Wine dataset** contains chemical analysis of 178 wines with 13 features:\n",
    "- Alcohol, Malic acid, Ash, Alkalinity of ash, Magnesium, Total phenols, Flavanoids, \n",
    "  Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315, Proline\n",
    "\n",
    "There are 3 wine cultivars (classes).\n",
    "\n",
    "**Questions:**\n",
    "1. Apply PCA and reduce to 2D\n",
    "2. Which original features are most important for PC1?\n",
    "3. Which original features are most important for PC2?\n",
    "4. Create a biplot showing both data points and feature loadings\n",
    "5. Can you identify which features distinguish the wine cultivars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Problem 6\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "feature_names_wine = wine.feature_names\n",
    "target_names_wine = wine.target_names\n",
    "\n",
    "print(\"Problem 6 Data Loaded: Wine Dataset\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of samples: {X_wine.shape[0]}\")\n",
    "print(f\"Number of features: {X_wine.shape[1]}\")\n",
    "print(f\"\\nFeatures:\")\n",
    "for i, name in enumerate(feature_names_wine, 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "print(f\"\\nTarget names: {target_names_wine}\")\n",
    "print(\"\\n‚Üí YOUR TASK: Apply PCA and analyze feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "# Steps:\n",
    "# 1. Standardize\n",
    "# 2. Apply PCA with 2 components\n",
    "# 3. Analyze loadings\n",
    "# 4. Create biplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>üìñ Show Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Solution for Problem 6\n",
    "\n",
    "# Step 1: Standardize\n",
    "scaler_wine = StandardScaler()\n",
    "X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca_wine = PCA(n_components=2)\n",
    "X_wine_pca = pca_wine.fit_transform(X_wine_scaled)\n",
    "\n",
    "print(\"Solution for Problem 6: Wine Dataset PCA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nExplained Variance:\")\n",
    "print(f\"  PC1: {pca_wine.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"  PC2: {pca_wine.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(f\"  Total: {sum(pca_wine.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# Step 3: Analyze loadings\n",
    "loadings_wine = pd.DataFrame(\n",
    "    pca_wine.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=feature_names_wine\n",
    ")\n",
    "loadings_wine['PC1_abs'] = np.abs(loadings_wine['PC1'])\n",
    "loadings_wine['PC2_abs'] = np.abs(loadings_wine['PC2'])\n",
    "\n",
    "print(\"\\nFeature Loadings:\")\n",
    "print(loadings_wine[['PC1', 'PC2']].to_string())\n",
    "\n",
    "# Question 2 & 3: Most important features\n",
    "print(\"\\nTop 3 features for PC1:\")\n",
    "top_pc1 = loadings_wine.nlargest(3, 'PC1_abs')[['PC1']]\n",
    "print(top_pc1.to_string())\n",
    "\n",
    "print(\"\\nTop 3 features for PC2:\")\n",
    "top_pc2 = loadings_wine.nlargest(3, 'PC2_abs')[['PC2']]\n",
    "print(top_pc2.to_string())\n",
    "\n",
    "# Step 4: Biplot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Scatter plot\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, (color, target_name) in enumerate(zip(colors, target_names_wine)):\n",
    "    mask = y_wine == i\n",
    "    axes[0].scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1], \n",
    "                    color=color, label=target_name, alpha=0.7, edgecolor='black', s=80)\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_title('Wine Dataset in PCA Space', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Biplot with loadings\n",
    "for i, (color, target_name) in enumerate(zip(colors, target_names_wine)):\n",
    "    mask = y_wine == i\n",
    "    axes[1].scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1], \n",
    "                    color=color, label=target_name, alpha=0.3, s=50)\n",
    "\n",
    "# Add loading vectors (scaled for visibility)\n",
    "scale = 3\n",
    "for i, feature in enumerate(feature_names_wine):\n",
    "    # Only show most important features to avoid clutter\n",
    "    if loadings_wine.iloc[i]['PC1_abs'] > 0.2 or loadings_wine.iloc[i]['PC2_abs'] > 0.2:\n",
    "        axes[1].arrow(0, 0, pca_wine.components_[0, i]*scale, pca_wine.components_[1, i]*scale,\n",
    "                      head_width=0.15, head_length=0.15, fc='black', ec='black', linewidth=2)\n",
    "        # Shorten feature names for readability\n",
    "        short_name = feature.replace('od280/od315_of_diluted_wines', 'OD ratio')\n",
    "        short_name = short_name.replace('nonflavanoid_phenols', 'nonflav_phenols')\n",
    "        axes[1].text(pca_wine.components_[0, i]*scale*1.15, pca_wine.components_[1, i]*scale*1.15,\n",
    "                     short_name, fontsize=9, ha='center', \n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "axes[1].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_title('PCA Biplot: Important Feature Loadings', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Question 5: Distinguishing features\n",
    "print(\"\\nQuestion 5: Features that distinguish wine cultivars:\")\n",
    "print(\"  ‚Üí PC1 (horizontal axis) is dominated by:\")\n",
    "print(\"    - Flavanoids, Total phenols, OD280/OD315 ratio\")\n",
    "print(\"    - These separate class_0 from classes_1 and class_2\")\n",
    "print(\"  ‚Üí PC2 (vertical axis) is influenced by:\")\n",
    "print(\"    - Color intensity, Proline, Malic acid\")\n",
    "print(\"    - These help separate class_1 from class_2\")\n",
    "```\n",
    "\n",
    "**Expected Answer:**\n",
    "- PC1 explains ~36% variance, PC2 explains ~19%\n",
    "- PC1: Flavanoids, Total phenols, OD280/OD315 ratio are most important\n",
    "- PC2: Color intensity, Proline, Malic acid are most important\n",
    "- The three wine cultivars are well-separated in 2D PCA space\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### Optimization with SciPy\n",
    "\n",
    "1. **Linear Programming** (`linprog`):\n",
    "   - For problems with linear objective and linear constraints\n",
    "   - Use `method='highs'` for best performance\n",
    "   - Remember to negate coefficients when maximizing\n",
    "\n",
    "2. **Nonlinear Optimization** (`minimize`):\n",
    "   - Choose method based on problem characteristics:\n",
    "     - `'BFGS'`: Fast, requires gradient (can approximate)\n",
    "     - `'Nelder-Mead'`: No gradient needed, robust but slower\n",
    "     - `'SLSQP'`: For constrained problems\n",
    "   - Always provide good initial guesses\n",
    "   - Use analytical gradients when possible for speed\n",
    "\n",
    "3. **Constraints**:\n",
    "   - Use `LinearConstraint` for linear constraints\n",
    "   - Use `NonlinearConstraint` for nonlinear constraints\n",
    "   - Bounds are specified separately\n",
    "\n",
    "### PCA for Dimensionality Reduction\n",
    "\n",
    "1. **Always standardize** data before PCA (using `StandardScaler`)\n",
    "\n",
    "2. **Choosing number of components**:\n",
    "   - Use scree plot (elbow method)\n",
    "   - Cumulative variance threshold (e.g., 90%, 95%)\n",
    "   - Cross-validation for downstream tasks\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - Explained variance ratio: how much information each PC captures\n",
    "   - Component loadings: which original features contribute to each PC\n",
    "   - Biplots: visualize both data and feature relationships\n",
    "\n",
    "4. **Applications**:\n",
    "   - Visualization (reduce to 2D or 3D)\n",
    "   - Feature extraction for machine learning\n",
    "   - Noise reduction\n",
    "   - Data compression\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Optimization**: Start simple, validate results, check convergence\n",
    "- **PCA**: Understand what variance means in your domain\n",
    "- **Both**: Always visualize your results!\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
