{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Means Clustering: A Comprehensive Tutorial\n",
        "## DA5400W - Foundations of Machine Learning\n",
        "### Instructor: Dr. Arun B Ayyar\n",
        "### IIT Madras\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a detailed explanation of:\n",
        "1. **Cluster Analysis** fundamentals\n",
        "2. **K-Means Algorithm** - theory, implementation, and visualization\n",
        "3. **Elbow Method** - determining the optimal number of clusters\n",
        "4. **Practical Applications** with real datasets\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand unsupervised learning and clustering\n",
        "- Master the K-means algorithm\n",
        "- Learn to select optimal k using the elbow method\n",
        "- Implement K-means from scratch and with scikit-learn\n",
        "- Apply K-means to real-world problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs, load_iris, load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")\n",
        "print(\"✓ Ready to start the K-means tutorial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: Introduction to Cluster Analysis\n",
        "\n",
        "## 1.1 What is Clustering?\n",
        "\n",
        "**Clustering** is an unsupervised learning technique that partitions data into groups (clusters) such that:\n",
        "- **High intra-cluster similarity**: Data points within the same cluster are similar\n",
        "- **Low inter-cluster similarity**: Data points in different clusters are dissimilar\n",
        "\n",
        "## 1.2 Key Definitions\n",
        "\n",
        "**Centroid** (μᵢ): The mean of all data points in a cluster\n",
        "$$\\mu_i = \\frac{1}{N_i} \\sum_{x_j \\in C_i} x_j$$\n",
        "\n",
        "**Radius** (rᵢ): Maximum distance from any data point to the centroid\n",
        "$$r_i = \\max_{x_j \\in C_i} ||x_j - \\mu_i||^2$$\n",
        "\n",
        "**Diameter** (dᵢ): Maximum pairwise distance among data points in a cluster\n",
        "$$d_i = \\max_{x_p, x_q \\in C_i} ||x_p - x_q||^2$$\n",
        "\n",
        "## 1.3 Applications\n",
        "\n",
        "- **Marketing**: Customer segmentation\n",
        "- **Documents**: Topic detection, information retrieval\n",
        "- **Image Processing**: Image segmentation\n",
        "- **Manufacturing**: Shop floor management\n",
        "- **Security**: Anomaly detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize labeled vs unlabeled data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=300, centers=3, n_features=2, \n",
        "                       cluster_std=0.8, random_state=42)\n",
        "\n",
        "# Labeled data (supervised learning)\n",
        "colors = ['red', 'blue', 'green']\n",
        "for i in range(3):\n",
        "    mask = y_true == i\n",
        "    axes[0].scatter(X[mask, 0], X[mask, 1], c=colors[i], \n",
        "                    label=f'Class {i}', s=60, alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Labeled Training Data\\n(Supervised Learning)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
        "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Unlabeled data (unsupervised learning)\n",
        "axes[1].scatter(X[:, 0], X[:, 1], c='darkred', s=60, alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title('Unlabeled Training Data\\n(Unsupervised Learning - Clustering)', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
        "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Clustering discovers hidden structure in unlabeled data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: K-Means Algorithm\n",
        "\n",
        "## 2.1 Why K-Means?\n",
        "\n",
        "- **Most widely used** clustering algorithm\n",
        "- **Simple** to understand and implement\n",
        "- **Reasonably fast** for small to medium datasets\n",
        "- Provides a quick estimate of the \"lay of the land\"\n",
        "- Available in most data mining tools\n",
        "\n",
        "## 2.2 The K-Means Algorithm\n",
        "\n",
        "**Objective**: Find K cluster centers that minimize the sum of squared distances of each data point to its nearest cluster center.\n",
        "\n",
        "**Objective Function (Inertia/Within-Cluster Sum of Squares)**:\n",
        "$$J = \\sum_{i=1}^{K} \\sum_{x_j \\in C_i} ||x_j - \\mu_i||^2$$\n",
        "\n",
        "**Algorithm Steps**:\n",
        "\n",
        "**Input**: Number of clusters k (predefined)\n",
        "\n",
        "1. **Initialize**: Choose k cluster centers (randomly or using heuristics)\n",
        "2. **Assignment Step**: Assign each data point to the nearest cluster center\n",
        "   $$C_i = \\{x_j : ||x_j - \\mu_i|| \\leq ||x_j - \\mu_l|| \\text{ for all } l\\}$$\n",
        "3. **Update Step**: Recompute cluster centers as the mean of assigned points\n",
        "   $$\\mu_i = \\frac{1}{|C_i|} \\sum_{x_j \\in C_i} x_j$$\n",
        "4. **Repeat**: Go to step 2 until convergence (centroids don't change significantly)\n",
        "\n",
        "**Convergence**: The algorithm converges when:\n",
        "- Centroids stop moving, OR\n",
        "- Cluster assignments don't change, OR\n",
        "- Maximum iterations reached"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 K-Means Visualization: Step-by-Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate simple 2D data for visualization\n",
        "X_demo, _ = make_blobs(n_samples=150, centers=3, n_features=2, \n",
        "                       cluster_std=0.6, random_state=42)\n",
        "\n",
        "# Initialize 3 random centroids\n",
        "k = 3\n",
        "np.random.seed(10)\n",
        "initial_centroids = X_demo[np.random.choice(X_demo.shape[0], k, replace=False)]\n",
        "\n",
        "def assign_clusters(X, centroids):\n",
        "    \"\"\"Assign each point to nearest centroid\"\"\"\n",
        "    distances = cdist(X, centroids, 'euclidean')\n",
        "    return np.argmin(distances, axis=1)\n",
        "\n",
        "def update_centroids(X, labels, k):\n",
        "    \"\"\"Compute new centroids as mean of assigned points\"\"\"\n",
        "    centroids = np.zeros((k, X.shape[1]))\n",
        "    for i in range(k):\n",
        "        centroids[i] = X[labels == i].mean(axis=0)\n",
        "    return centroids\n",
        "\n",
        "# Visualize iterations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "centroids = initial_centroids.copy()\n",
        "colors_map = ['red', 'blue', 'green']\n",
        "\n",
        "for iteration in range(6):\n",
        "    ax = axes[iteration]\n",
        "    \n",
        "    if iteration == 0:\n",
        "        # Initial state\n",
        "        ax.scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=50, alpha=0.5, edgecolor='black')\n",
        "        ax.scatter(centroids[:, 0], centroids[:, 1], c=colors_map, \n",
        "                   marker='*', s=500, edgecolor='black', linewidth=2, label='Initial Centroids')\n",
        "        ax.set_title(f'Iteration 0: Initialize Centroids', fontsize=13, fontweight='bold')\n",
        "    else:\n",
        "        # Assignment step\n",
        "        labels = assign_clusters(X_demo, centroids)\n",
        "        \n",
        "        # Plot points colored by cluster\n",
        "        for i in range(k):\n",
        "            mask = labels == i\n",
        "            ax.scatter(X_demo[mask, 0], X_demo[mask, 1], c=colors_map[i], \n",
        "                       s=50, alpha=0.6, edgecolor='black')\n",
        "        \n",
        "        # Plot centroids\n",
        "        ax.scatter(centroids[:, 0], centroids[:, 1], c=colors_map, \n",
        "                   marker='*', s=500, edgecolor='black', linewidth=2)\n",
        "        \n",
        "        ax.set_title(f'Iteration {iteration}: Assign & Update', fontsize=13, fontweight='bold')\n",
        "        \n",
        "        # Update centroids for next iteration\n",
        "        centroids = update_centroids(X_demo, labels, k)\n",
        "    \n",
        "    ax.set_xlabel('Feature 1', fontsize=11)\n",
        "    ax.set_ylabel('Feature 2', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('K-Means Algorithm: Iterative Process', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nK-Means iteratively:\")\n",
        "print(\"1. Assigns points to nearest centroid (Assignment Step)\")\n",
        "print(\"2. Updates centroids as mean of assigned points (Update Step)\")\n",
        "print(\"3. Repeats until convergence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 K-Means from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KMeansFromScratch:\n",
        "    \"\"\"\n",
        "    K-Means clustering implementation from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=3, max_iters=100, tol=1e-4, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.centroids = None\n",
        "        self.labels_ = None\n",
        "        self.inertia_ = None\n",
        "        \n",
        "    def fit(self, X):\n",
        "        \"\"\"Fit K-Means to data X\"\"\"\n",
        "        if self.random_state is not None:\n",
        "            np.random.seed(self.random_state)\n",
        "        \n",
        "        # Initialize centroids randomly\n",
        "        idx = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
        "        self.centroids = X[idx].copy()\n",
        "        \n",
        "        for iteration in range(self.max_iters):\n",
        "            # Assignment step\n",
        "            distances = cdist(X, self.centroids, 'euclidean')\n",
        "            self.labels_ = np.argmin(distances, axis=1)\n",
        "            \n",
        "            # Update step\n",
        "            new_centroids = np.zeros_like(self.centroids)\n",
        "            for i in range(self.n_clusters):\n",
        "                if np.sum(self.labels_ == i) > 0:\n",
        "                    new_centroids[i] = X[self.labels_ == i].mean(axis=0)\n",
        "                else:\n",
        "                    new_centroids[i] = self.centroids[i]\n",
        "            \n",
        "            # Check convergence\n",
        "            centroid_shift = np.linalg.norm(new_centroids - self.centroids)\n",
        "            self.centroids = new_centroids\n",
        "            \n",
        "            if centroid_shift < self.tol:\n",
        "                print(f\"Converged at iteration {iteration+1}\")\n",
        "                break\n",
        "        \n",
        "        # Calculate inertia\n",
        "        self.inertia_ = 0\n",
        "        for i in range(self.n_clusters):\n",
        "            cluster_points = X[self.labels_ == i]\n",
        "            self.inertia_ += np.sum((cluster_points - self.centroids[i])**2)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict cluster labels for X\"\"\"\n",
        "        distances = cdist(X, self.centroids, 'euclidean')\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "# Test the implementation\n",
        "kmeans_scratch = KMeansFromScratch(n_clusters=3, random_state=42)\n",
        "kmeans_scratch.fit(X_demo)\n",
        "\n",
        "print(f\"\\nFinal Inertia: {kmeans_scratch.inertia_:.2f}\")\n",
        "print(f\"Final Centroids:\\n{kmeans_scratch.centroids}\")\n",
        "print(\"\\n✓ K-Means from scratch implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: The Elbow Method for Selecting k\n",
        "\n",
        "## 3.1 The Problem: Choosing k\n",
        "\n",
        "K-Means requires us to specify k (number of clusters) in advance. But how do we choose the right k?\n",
        "\n",
        "**The Elbow Method** is a heuristic approach to find the optimal k.\n",
        "\n",
        "## 3.2 How the Elbow Method Works\n",
        "\n",
        "1. Run K-Means for different values of k (e.g., k = 1 to 10)\n",
        "2. For each k, calculate the **Within-Cluster Sum of Squares (WCSS)** or **Inertia**:\n",
        "   $$\\text{WCSS} = \\sum_{i=1}^{K} \\sum_{x_j \\in C_i} ||x_j - \\mu_i||^2$$\n",
        "3. Plot k vs WCSS\n",
        "4. Look for the \"elbow\" - the point where WCSS starts decreasing more slowly\n",
        "\n",
        "**Intuition**:\n",
        "- As k increases, WCSS decreases (more clusters = tighter fit)\n",
        "- At k = n (number of data points), WCSS = 0\n",
        "- The \"elbow\" represents a good trade-off between:\n",
        "  - **Model complexity** (number of clusters)\n",
        "  - **Model fit** (how well clusters represent data)\n",
        "\n",
        "## 3.3 Elbow Method Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data with clear clusters\n",
        "X_elbow, y_elbow = make_blobs(n_samples=300, centers=4, n_features=2, \n",
        "                               cluster_std=0.7, random_state=42)\n",
        "\n",
        "# Calculate WCSS for different k values\n",
        "k_range = range(1, 11)\n",
        "wcss = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_elbow)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    \n",
        "    # Silhouette score (only for k >= 2)\n",
        "    if k >= 2:\n",
        "        silhouette_scores.append(silhouette_score(X_elbow, kmeans.labels_))\n",
        "    else:\n",
        "        silhouette_scores.append(0)\n",
        "\n",
        "# Plot elbow curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# WCSS (Elbow) plot\n",
        "axes[0].plot(k_range, wcss, 'bo-', linewidth=2, markersize=10)\n",
        "axes[0].axvline(x=4, color='red', linestyle='--', linewidth=2, label='Optimal k=4 (Elbow)')\n",
        "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "axes[0].set_ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
        "axes[0].set_title('Elbow Method: Finding Optimal k', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].set_xticks(k_range)\n",
        "\n",
        "# Silhouette score plot\n",
        "axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=10)\n",
        "axes[1].axvline(x=4, color='red', linestyle='--', linewidth=2, label='Optimal k=4')\n",
        "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
        "axes[1].set_title('Silhouette Score: Validating k', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].set_xticks(k_range)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nElbow Method Results:\")\n",
        "print(\"=\"*60)\n",
        "for k, w, s in zip(k_range, wcss, silhouette_scores):\n",
        "    print(f\"k={k:2d}  WCSS={w:8.2f}  Silhouette={s:.4f}\")\n",
        "\n",
        "print(\"\\n→ The 'elbow' occurs at k=4\")\n",
        "print(\"→ Silhouette score is also highest at k=4\")\n",
        "print(\"→ Optimal number of clusters: k=4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Understanding the Elbow\n",
        "\n",
        "**What to look for:**\n",
        "- The \"elbow\" is the point where the curve bends sharply\n",
        "- Before the elbow: Adding clusters significantly reduces WCSS\n",
        "- After the elbow: Adding clusters provides diminishing returns\n",
        "\n",
        "**Limitations:**\n",
        "- The elbow may not always be clear\n",
        "- Subjective interpretation\n",
        "- Works best when clusters are well-separated\n",
        "\n",
        "**Alternative Metrics:**\n",
        "- **Silhouette Score**: Measures how similar a point is to its own cluster vs other clusters\n",
        "  - Range: [-1, 1]\n",
        "  - Higher is better\n",
        "- **Davies-Bouldin Index**: Ratio of within-cluster to between-cluster distances\n",
        "  - Lower is better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: Practical Example - Customer Segmentation\n",
        "\n",
        "## 4.1 Problem Setup\n",
        "\n",
        "Let's apply K-Means to a realistic customer segmentation problem using synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic customer data\n",
        "np.random.seed(42)\n",
        "n_customers = 400\n",
        "\n",
        "# Features: Annual Income (k$), Spending Score (1-100)\n",
        "# Create 5 distinct customer segments\n",
        "segment1 = np.random.randn(80, 2) * [5, 8] + [25, 25]   # Low income, low spending\n",
        "segment2 = np.random.randn(80, 2) * [5, 8] + [25, 75]   # Low income, high spending\n",
        "segment3 = np.random.randn(80, 2) * [5, 8] + [50, 50]   # Medium income, medium spending\n",
        "segment4 = np.random.randn(80, 2) * [5, 8] + [75, 25]   # High income, low spending\n",
        "segment5 = np.random.randn(80, 2) * [5, 8] + [75, 75]   # High income, high spending\n",
        "\n",
        "X_customers = np.vstack([segment1, segment2, segment3, segment4, segment5])\n",
        "\n",
        "# Create DataFrame\n",
        "df_customers = pd.DataFrame(X_customers, columns=['Annual_Income_k$', 'Spending_Score'])\n",
        "\n",
        "print(\"Customer Data:\")\n",
        "print(\"=\"*60)\n",
        "print(df_customers.describe())\n",
        "print(f\"\\nTotal customers: {len(df_customers)}\")\n",
        "\n",
        "# Visualize raw data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df_customers['Annual_Income_k$'], df_customers['Spending_Score'], \n",
        "            c='darkblue', s=60, alpha=0.6, edgecolor='black')\n",
        "plt.xlabel('Annual Income (k$)', fontsize=12)\n",
        "plt.ylabel('Spending Score (1-100)', fontsize=12)\n",
        "plt.title('Customer Data: Annual Income vs Spending Score', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Apply Elbow Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_customers_scaled = scaler.fit_transform(X_customers)\n",
        "\n",
        "# Elbow method\n",
        "k_range = range(1, 11)\n",
        "wcss_customers = []\n",
        "silhouette_customers = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_customers_scaled)\n",
        "    wcss_customers.append(kmeans.inertia_)\n",
        "    \n",
        "    if k >= 2:\n",
        "        silhouette_customers.append(silhouette_score(X_customers_scaled, kmeans.labels_))\n",
        "    else:\n",
        "        silhouette_customers.append(0)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "axes[0].plot(k_range, wcss_customers, 'bo-', linewidth=2, markersize=10)\n",
        "axes[0].axvline(x=5, color='red', linestyle='--', linewidth=2, label='Optimal k=5')\n",
        "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "axes[0].set_ylabel('WCSS', fontsize=12)\n",
        "axes[0].set_title('Elbow Method: Customer Segmentation', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].set_xticks(k_range)\n",
        "\n",
        "axes[1].plot(k_range, silhouette_customers, 'go-', linewidth=2, markersize=10)\n",
        "axes[1].axvline(x=5, color='red', linestyle='--', linewidth=2, label='Optimal k=5')\n",
        "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
        "axes[1].set_title('Silhouette Score: Customer Segmentation', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].set_xticks(k_range)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n→ Optimal k = 5 customer segments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Final Clustering with k=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply K-Means with k=5\n",
        "kmeans_final = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "df_customers['Cluster'] = kmeans_final.fit_predict(X_customers_scaled)\n",
        "\n",
        "# Get centroids in original scale\n",
        "centroids_scaled = kmeans_final.cluster_centers_\n",
        "centroids_original = scaler.inverse_transform(centroids_scaled)\n",
        "\n",
        "# Visualize clusters\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "segment_names = ['Low Income\\nLow Spending', 'Low Income\\nHigh Spending', \n",
        "                 'Medium Income\\nMedium Spending', 'High Income\\nLow Spending', \n",
        "                 'High Income\\nHigh Spending']\n",
        "\n",
        "for i in range(5):\n",
        "    cluster_data = df_customers[df_customers['Cluster'] == i]\n",
        "    plt.scatter(cluster_data['Annual_Income_k$'], cluster_data['Spending_Score'], \n",
        "                c=colors[i], label=f'Segment {i+1}: {segment_names[i]}', \n",
        "                s=80, alpha=0.6, edgecolor='black')\n",
        "\n",
        "# Plot centroids\n",
        "plt.scatter(centroids_original[:, 0], centroids_original[:, 1], \n",
        "            c='black', marker='*', s=500, edgecolor='white', linewidth=2, \n",
        "            label='Centroids', zorder=10)\n",
        "\n",
        "plt.xlabel('Annual Income (k$)', fontsize=13)\n",
        "plt.ylabel('Spending Score (1-100)', fontsize=13)\n",
        "plt.title('Customer Segmentation: 5 Distinct Segments', fontsize=15, fontweight='bold')\n",
        "plt.legend(fontsize=10, loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cluster statistics\n",
        "print(\"\\nCluster Statistics:\")\n",
        "print(\"=\"*80)\n",
        "for i in range(5):\n",
        "    cluster_data = df_customers[df_customers['Cluster'] == i]\n",
        "    print(f\"\\nSegment {i+1}: {segment_names[i]}\")\n",
        "    print(f\"  Size: {len(cluster_data)} customers ({len(cluster_data)/len(df_customers)*100:.1f}%)\")\n",
        "    print(f\"  Avg Income: ${cluster_data['Annual_Income_k$'].mean():.1f}k\")\n",
        "    print(f\"  Avg Spending Score: {cluster_data['Spending_Score'].mean():.1f}\")\n",
        "    print(f\"  Centroid: Income=${centroids_original[i, 0]:.1f}k, Score={centroids_original[i, 1]:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Business Insights\n",
        "\n",
        "Based on the customer segmentation:\n",
        "\n",
        "**Segment 1: Low Income, Low Spending**\n",
        "- Strategy: Budget-friendly products, discounts, loyalty programs\n",
        "\n",
        "**Segment 2: Low Income, High Spending**\n",
        "- Strategy: Credit options, installment plans, value deals\n",
        "\n",
        "**Segment 3: Medium Income, Medium Spending**\n",
        "- Strategy: Balanced product mix, seasonal promotions\n",
        "\n",
        "**Segment 4: High Income, Low Spending**\n",
        "- Strategy: Premium products, exclusive offers, personalized service\n",
        "\n",
        "**Segment 5: High Income, High Spending**\n",
        "- Strategy: VIP treatment, luxury products, premium experiences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 5: Real Dataset Example - Iris Dataset\n",
        "\n",
        "## 5.1 Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "df_iris = pd.DataFrame(X_iris, columns=feature_names)\n",
        "df_iris['species'] = iris.target_names[y_iris]\n",
        "\n",
        "print(\"Iris Dataset:\")\n",
        "print(\"=\"*60)\n",
        "print(df_iris.head(10))\n",
        "print(f\"\\nShape: {X_iris.shape}\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "print(f\"Species: {iris.target_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Apply K-Means (Unsupervised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize\n",
        "scaler_iris = StandardScaler()\n",
        "X_iris_scaled = scaler_iris.fit_transform(X_iris)\n",
        "\n",
        "# Elbow method\n",
        "k_range = range(1, 11)\n",
        "wcss_iris = []\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_iris_scaled)\n",
        "    wcss_iris.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, wcss_iris, 'bo-', linewidth=2, markersize=10)\n",
        "plt.axvline(x=3, color='red', linestyle='--', linewidth=2, label='Optimal k=3')\n",
        "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
        "plt.ylabel('WCSS', fontsize=12)\n",
        "plt.title('Elbow Method: Iris Dataset', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(fontsize=11)\n",
        "plt.xticks(k_range)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Apply K-Means with k=3\n",
        "kmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "clusters_iris = kmeans_iris.fit_predict(X_iris_scaled)\n",
        "\n",
        "print(f\"\\nOptimal k = 3 (matches the true number of species!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Compare Clusters with True Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize: Petal Length vs Petal Width\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# True labels\n",
        "colors_true = ['red', 'blue', 'green']\n",
        "for i, species in enumerate(iris.target_names):\n",
        "    mask = y_iris == i\n",
        "    axes[0].scatter(X_iris[mask, 2], X_iris[mask, 3], c=colors_true[i], \n",
        "                    label=species, s=80, alpha=0.7, edgecolor='black')\n",
        "axes[0].set_xlabel('Petal Length (cm)', fontsize=12)\n",
        "axes[0].set_ylabel('Petal Width (cm)', fontsize=12)\n",
        "axes[0].set_title('True Species Labels', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# K-Means clusters\n",
        "colors_cluster = ['orange', 'purple', 'cyan']\n",
        "for i in range(3):\n",
        "    mask = clusters_iris == i\n",
        "    axes[1].scatter(X_iris[mask, 2], X_iris[mask, 3], c=colors_cluster[i], \n",
        "                    label=f'Cluster {i}', s=80, alpha=0.7, edgecolor='black')\n",
        "\n",
        "# Plot centroids\n",
        "centroids_iris = scaler_iris.inverse_transform(kmeans_iris.cluster_centers_)\n",
        "axes[1].scatter(centroids_iris[:, 2], centroids_iris[:, 3], \n",
        "                c='black', marker='*', s=500, edgecolor='white', linewidth=2, \n",
        "                label='Centroids', zorder=10)\n",
        "\n",
        "axes[1].set_xlabel('Petal Length (cm)', fontsize=12)\n",
        "axes[1].set_ylabel('Petal Width (cm)', fontsize=12)\n",
        "axes[1].set_title('K-Means Clusters (k=3)', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Accuracy (with best label mapping)\n",
        "from scipy.stats import mode\n",
        "labels_mapped = np.zeros_like(clusters_iris)\n",
        "for i in range(3):\n",
        "    mask = clusters_iris == i\n",
        "    labels_mapped[mask] = mode(y_iris[mask], keepdims=True)[0][0]\n",
        "\n",
        "accuracy = np.mean(labels_mapped == y_iris)\n",
        "print(f\"\\nClustering Accuracy (with best mapping): {accuracy*100:.1f}%\")\n",
        "print(\"\\n→ K-Means successfully discovered the 3 species without labels!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 6: K-Means Limitations and Best Practices\n",
        "\n",
        "## 6.1 Limitations\n",
        "\n",
        "**1. Requires k to be specified**\n",
        "- Solution: Use elbow method, silhouette analysis\n",
        "\n",
        "**2. Sensitive to initialization**\n",
        "- Solution: Run multiple times with different initializations (`n_init` parameter)\n",
        "\n",
        "**3. Assumes spherical clusters**\n",
        "- Problem: Fails with elongated or irregular shapes\n",
        "- Solution: Consider DBSCAN, Gaussian Mixture Models\n",
        "\n",
        "**4. Sensitive to outliers**\n",
        "- Solution: Remove outliers, use K-medoids (more robust)\n",
        "\n",
        "**5. Sensitive to scale**\n",
        "- Solution: Always standardize features\n",
        "\n",
        "## 6.2 Best Practices\n",
        "\n",
        "1. **Standardize features** before clustering\n",
        "2. **Use multiple initializations** (`n_init=10` or more)\n",
        "3. **Try different k values** and use elbow method\n",
        "4. **Validate with multiple metrics** (silhouette, Davies-Bouldin)\n",
        "5. **Visualize results** when possible\n",
        "6. **Consider domain knowledge** when interpreting clusters\n",
        "7. **Check for outliers** before clustering\n",
        "\n",
        "## 6.3 When to Use K-Means\n",
        "\n",
        "**Good for:**\n",
        "- Large datasets (fast and scalable)\n",
        "- Spherical, well-separated clusters\n",
        "- When k is known or can be estimated\n",
        "- Exploratory data analysis\n",
        "\n",
        "**Not good for:**\n",
        "- Non-spherical clusters\n",
        "- Clusters with very different sizes\n",
        "- Noisy data with many outliers\n",
        "- When cluster density varies significantly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 7: Summary and Key Takeaways\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "1. **Cluster Analysis** is unsupervised learning for discovering patterns in unlabeled data\n",
        "\n",
        "2. **K-Means Algorithm**:\n",
        "   - Iteratively assigns points to nearest centroid\n",
        "   - Updates centroids as mean of assigned points\n",
        "   - Minimizes within-cluster sum of squares (WCSS)\n",
        "   - Simple, fast, and widely used\n",
        "\n",
        "3. **Elbow Method**:\n",
        "   - Plot k vs WCSS\n",
        "   - Look for the \"elbow\" point\n",
        "   - Represents optimal trade-off between complexity and fit\n",
        "\n",
        "4. **Practical Applications**:\n",
        "   - Customer segmentation\n",
        "   - Image segmentation\n",
        "   - Document clustering\n",
        "   - Anomaly detection\n",
        "\n",
        "5. **Best Practices**:\n",
        "   - Standardize features\n",
        "   - Use multiple initializations\n",
        "   - Validate with multiple metrics\n",
        "   - Visualize and interpret results\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- K-Means++: Improved initialization method\n",
        "- Mini-Batch K-Means: Faster variant for large datasets\n",
        "- DBSCAN: Density-based clustering (handles non-spherical clusters)\n",
        "- Hierarchical Clustering: Creates tree of clusters\n",
        "- Gaussian Mixture Models: Probabilistic clustering\n",
        "\n",
        "---\n",
        "\n",
        "**End of Tutorial**\n",
        "\n",
        "Thank you for learning K-Means clustering! Practice with different datasets to master the technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 8: Hierarchical Clustering\n",
        "\n",
        "Hierarchical clustering builds a tree of clusters called a **dendrogram**.\n",
        "\n",
        "## 8.1 Introduction\n",
        "\n",
        "**Two Approaches:**\n",
        "- **Agglomerative (AGNES)**: Bottom-up, start with individual points, merge closest\n",
        "- **Divisive (DIANA)**: Top-down, start with one cluster, divide based on criterion\n",
        "\n",
        "**Advantages:**\n",
        "- No need to specify k in advance\n",
        "- Produces a dendrogram (visual hierarchy)\n",
        "- Can capture nested clusters\n",
        "\n",
        "**Disadvantages:**\n",
        "- Computationally expensive: O(n³) or O(n² log n)\n",
        "- Cannot undo merges/splits\n",
        "- Sensitive to noise and outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "X_hier = np.vstack([\n",
        "    np.random.randn(30, 2) + [2, 2],\n",
        "    np.random.randn(30, 2) + [8, 2],\n",
        "    np.random.randn(30, 2) + [5, 8]\n",
        "])\n",
        "\n",
        "print(f\"Generated {len(X_hier)} points for hierarchical clustering\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.2 Understanding Dendrograms\n",
        "\n",
        "A dendrogram shows:\n",
        "- Each vertical line represents a cluster\n",
        "- Height shows the distance at which clusters merge\n",
        "- Cutting at different heights gives different numbers of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dendrogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "linkage_matrix = linkage(X_hier, method='ward')\n",
        "dendrogram(linkage_matrix)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.axhline(y=15, color='r', linestyle='--', label='Cut at height 15 (3 clusters)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"How to read the dendrogram:\")\n",
        "print(\"• Each vertical line represents a cluster\")\n",
        "print(\"• Height shows the distance at which clusters merge\")\n",
        "print(\"• Horizontal red line shows where we cut to get 3 clusters\")\n",
        "print(\"• Cutting at different heights gives different numbers of clusters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.3 Linkage Methods\n",
        "\n",
        "Different ways to measure distance between clusters:\n",
        "\n",
        "1. **Single Linkage**: Minimum distance between any two points\n",
        "   - dist(Ci, Cj) = min{dist(xp, xq) : xp ∈ Ci, xq ∈ Cj}\n",
        "   - Tends to create elongated clusters (chain effect)\n",
        "\n",
        "2. **Complete Linkage**: Maximum distance between any two points\n",
        "   - dist(Ci, Cj) = max{dist(xp, xq) : xp ∈ Ci, xq ∈ Cj}\n",
        "   - Creates compact, spherical clusters\n",
        "\n",
        "3. **Average Linkage**: Average distance between all pairs\n",
        "   - dist(Ci, Cj) = (1/(Ni×Nj)) × Σ dist(xp, xq)\n",
        "   - Balanced approach\n",
        "\n",
        "4. **Ward Linkage**: Minimizes within-cluster variance\n",
        "   - Most recommended for general use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare linkage methods\n",
        "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, method in enumerate(linkage_methods):\n",
        "    agg = AgglomerativeClustering(n_clusters=3, linkage=method)\n",
        "    labels = agg.fit_predict(X_hier)\n",
        "    \n",
        "    axes[idx].scatter(X_hier[:, 0], X_hier[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "    axes[idx].set_title(f'{method.capitalize()} Linkage', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Linkage Method Characteristics:\")\n",
        "print(\"• Single: Tends to create elongated clusters (chain effect)\")\n",
        "print(\"• Complete: Creates compact, spherical clusters\")\n",
        "print(\"• Average: Balanced approach\")\n",
        "print(\"• Ward: Minimizes variance (recommended)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.4 Dendrogram Comparison\n",
        "\n",
        "Let's compare dendrograms for different linkage methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare dendrograms\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, method in enumerate(linkage_methods):\n",
        "    linkage_matrix = linkage(X_hier, method=method)\n",
        "    dendrogram(linkage_matrix, ax=axes[idx])\n",
        "    axes[idx].set_title(f'{method.capitalize()} Linkage Dendrogram', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Sample Index')\n",
        "    axes[idx].set_ylabel('Distance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how different linkage methods produce different tree structures\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.5 Choosing Number of Clusters\n",
        "\n",
        "Use silhouette score to find optimal number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find optimal k for hierarchical clustering\n",
        "k_range = range(2, 8)\n",
        "silhouette_scores_hier = []\n",
        "\n",
        "for k in k_range:\n",
        "    agg = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
        "    labels = agg.fit_predict(X_hier)\n",
        "    score = silhouette_score(X_hier, labels)\n",
        "    silhouette_scores_hier.append(score)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_range, silhouette_scores_hier, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Clusters', fontsize=12)\n",
        "plt.ylabel('Silhouette Score', fontsize=12)\n",
        "plt.title('Hierarchical Clustering: Silhouette Score vs k', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_k_hier = k_range[np.argmax(silhouette_scores_hier)]\n",
        "print(f\"Optimal k: {best_k_hier}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.6 Hierarchical Clustering from Scratch\n",
        "\n",
        "Let's implement agglomerative clustering from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HierarchicalClusteringFromScratch:\n",
        "    def __init__(self, n_clusters=3, linkage='single'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.linkage = linkage\n",
        "        self.labels_ = None\n",
        "    \n",
        "    def fit_predict(self, X):\n",
        "        n_samples = len(X)\n",
        "        # Initialize: each point is its own cluster\n",
        "        clusters = [[i] for i in range(n_samples)]\n",
        "        \n",
        "        # Compute initial distance matrix\n",
        "        from scipy.spatial.distance import pdist, squareform\n",
        "        dist_matrix = squareform(pdist(X, metric='euclidean'))\n",
        "        \n",
        "        # Merge until we have desired number of clusters\n",
        "        while len(clusters) > self.n_clusters:\n",
        "            # Find closest pair of clusters\n",
        "            min_dist = float('inf')\n",
        "            merge_i, merge_j = 0, 1\n",
        "            \n",
        "            for i in range(len(clusters)):\n",
        "                for j in range(i + 1, len(clusters)):\n",
        "                    dist = self._cluster_distance(clusters[i], clusters[j], dist_matrix)\n",
        "                    if dist < min_dist:\n",
        "                        min_dist = dist\n",
        "                        merge_i, merge_j = i, j\n",
        "            \n",
        "            # Merge the two closest clusters\n",
        "            clusters[merge_i].extend(clusters[merge_j])\n",
        "            del clusters[merge_j]\n",
        "        \n",
        "        # Create labels array\n",
        "        labels = np.zeros(n_samples, dtype=int)\n",
        "        for cluster_id, cluster_points in enumerate(clusters):\n",
        "            for point_idx in cluster_points:\n",
        "                labels[point_idx] = cluster_id\n",
        "        \n",
        "        self.labels_ = labels\n",
        "        return labels\n",
        "    \n",
        "    def _cluster_distance(self, cluster1, cluster2, dist_matrix):\n",
        "        if self.linkage == 'single':\n",
        "            # Minimum distance\n",
        "            return min(dist_matrix[i, j] for i in cluster1 for j in cluster2)\n",
        "        elif self.linkage == 'complete':\n",
        "            # Maximum distance\n",
        "            return max(dist_matrix[i, j] for i in cluster1 for j in cluster2)\n",
        "        elif self.linkage == 'average':\n",
        "            # Average distance\n",
        "            distances = [dist_matrix[i, j] for i in cluster1 for j in cluster2]\n",
        "            return np.mean(distances)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown linkage: {self.linkage}\")\n",
        "\n",
        "# Test our implementation\n",
        "hier_scratch = HierarchicalClusteringFromScratch(n_clusters=3, linkage='average')\n",
        "labels_scratch = hier_scratch.fit_predict(X_hier)\n",
        "\n",
        "# Compare with scikit-learn\n",
        "hier_sklearn = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
        "labels_sklearn = hier_sklearn.fit_predict(X_hier)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].scatter(X_hier[:, 0], X_hier[:, 1], c=labels_scratch, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[0].set_title('From Scratch', fontweight='bold')\n",
        "axes[1].scatter(X_hier[:, 0], X_hier[:, 1], c=labels_sklearn, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[1].set_title('Scikit-learn', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"From-scratch implementation validated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.7 Summary: Hierarchical Clustering\n",
        "\n",
        "**When to use:**\n",
        "- Need hierarchy of clusters\n",
        "- Don't know k in advance\n",
        "- Small to medium datasets\n",
        "\n",
        "**Linkage recommendations:**\n",
        "- **Ward**: Best for most cases\n",
        "- **Complete**: For compact clusters\n",
        "- **Average**: Balanced approach\n",
        "- **Single**: Avoid unless you need chain-like clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 9: DBSCAN (Density-Based Clustering)\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds clusters based on density.\n",
        "\n",
        "## 9.1 Key Concepts\n",
        "\n",
        "**Parameters:**\n",
        "- **ε (epsilon)**: Maximum distance between two points to be considered neighbors\n",
        "- **MinPts**: Minimum number of points to form a dense region\n",
        "\n",
        "**Point Types:**\n",
        "- **Core point**: Has at least MinPts neighbors within ε\n",
        "- **Border point**: Within ε of a core point but has fewer than MinPts neighbors\n",
        "- **Noise point**: Neither core nor border\n",
        "\n",
        "**Advantages:**\n",
        "- Finds arbitrary-shaped clusters\n",
        "- Identifies outliers as noise\n",
        "- No need to specify number of clusters\n",
        "\n",
        "**Disadvantages:**\n",
        "- Sensitive to ε and MinPts parameters\n",
        "- Struggles with varying densities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "\n",
        "# Create datasets where K-Means fails\n",
        "X_moons, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
        "X_circles, _ = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=42)\n",
        "\n",
        "print(\"Created challenging datasets for DBSCAN demonstration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.2 DBSCAN vs K-Means\n",
        "\n",
        "Let's see how DBSCAN handles non-spherical clusters where K-Means fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "datasets = [(X_moons, 'Moons'), (X_circles, 'Circles')]\n",
        "\n",
        "for row, (X, name) in enumerate(datasets):\n",
        "    # Original data\n",
        "    axes[row, 0].scatter(X[:, 0], X[:, 1], s=50, alpha=0.6)\n",
        "    axes[row, 0].set_title(f'{name} Dataset', fontweight='bold')\n",
        "    \n",
        "    # K-Means\n",
        "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "    labels_km = kmeans.fit_predict(X)\n",
        "    axes[row, 1].scatter(X[:, 0], X[:, 1], c=labels_km, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "    axes[row, 1].set_title(f'K-Means (Fails)', fontweight='bold')\n",
        "    \n",
        "    # DBSCAN\n",
        "    dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "    labels_db = dbscan.fit_predict(X)\n",
        "    axes[row, 2].scatter(X[:, 0], X[:, 1], c=labels_db, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "    # Mark noise points\n",
        "    noise = X[labels_db == -1]\n",
        "    axes[row, 2].scatter(noise[:, 0], noise[:, 1], c='red', marker='x', s=100, label='Noise')\n",
        "    axes[row, 2].set_title(f'DBSCAN (Succeeds)', fontweight='bold')\n",
        "    axes[row, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Observations:\")\n",
        "print(\"• K-Means assumes spherical clusters and fails on non-convex shapes\")\n",
        "print(\"• DBSCAN successfully identifies the true structure\")\n",
        "print(\"• DBSCAN identifies outliers as noise (red X markers)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.3 DBSCAN Algorithm Step-by-Step\n",
        "\n",
        "**Algorithm:**\n",
        "1. For each point p:\n",
        "   - Find all points within ε distance (neighbors)\n",
        "   - If neighbors >= MinPts, p is a core point\n",
        "2. For each core point:\n",
        "   - Create a cluster with all density-reachable points\n",
        "3. Assign border points to nearby clusters\n",
        "4. Mark remaining points as noise\n",
        "\n",
        "**Time Complexity:** O(n log n) with spatial indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize DBSCAN concepts\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Create simple dataset\n",
        "np.random.seed(42)\n",
        "X_demo = np.random.randn(50, 2)\n",
        "\n",
        "# Run DBSCAN\n",
        "dbscan_demo = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels_demo = dbscan_demo.fit_predict(X_demo)\n",
        "\n",
        "# Identify point types\n",
        "core_samples = np.zeros(len(X_demo), dtype=bool)\n",
        "core_samples[dbscan_demo.core_sample_indices_] = True\n",
        "noise = labels_demo == -1\n",
        "border = ~core_samples & ~noise\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Clusters\n",
        "axes[0].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_demo, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
        "axes[0].scatter(X_demo[noise, 0], X_demo[noise, 1], c='red', marker='x', s=200, linewidths=3, label='Noise')\n",
        "axes[0].set_title('DBSCAN Clusters', fontweight='bold')\n",
        "axes[0].legend()\n",
        "\n",
        "# Point types\n",
        "axes[1].scatter(X_demo[core_samples, 0], X_demo[core_samples, 1], c='blue', s=100, label='Core', edgecolors='k')\n",
        "axes[1].scatter(X_demo[border, 0], X_demo[border, 1], c='orange', s=100, marker='s', label='Border', edgecolors='k')\n",
        "axes[1].scatter(X_demo[noise, 0], X_demo[noise, 1], c='red', s=100, marker='x', label='Noise', linewidths=3)\n",
        "axes[1].set_title('Point Classification', fontweight='bold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Core points: {core_samples.sum()}\")\n",
        "print(f\"Border points: {border.sum()}\")\n",
        "print(f\"Noise points: {noise.sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.4 Parameter Selection: K-Distance Graph\n",
        "\n",
        "The **K-distance graph** helps choose ε:\n",
        "1. For each point, find distance to k-th nearest neighbor\n",
        "2. Sort distances in ascending order\n",
        "3. Plot the sorted distances\n",
        "4. Look for the \"elbow\" - sharp increase indicates good ε"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-distance graph for parameter selection\n",
        "k = 5  # MinPts\n",
        "nbrs = NearestNeighbors(n_neighbors=k).fit(X_moons)\n",
        "distances, indices = nbrs.kneighbors(X_moons)\n",
        "distances = np.sort(distances[:, k-1], axis=0)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(distances, linewidth=2)\n",
        "plt.ylabel('K-th Nearest Neighbor Distance', fontsize=12)\n",
        "plt.xlabel('Points sorted by distance', fontsize=12)\n",
        "plt.title('K-Distance Graph (k=5)', fontsize=14, fontweight='bold')\n",
        "plt.axhline(y=0.3, color='r', linestyle='--', label='Suggested ε = 0.3')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The elbow point suggests ε ≈ 0.3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.5 Effect of Parameters\n",
        "\n",
        "Let's see how different ε values affect clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different epsilon values\n",
        "eps_values = [0.1, 0.2, 0.3, 0.4]\n",
        "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
        "\n",
        "for idx, eps in enumerate(eps_values):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "    labels = dbscan.fit_predict(X_moons)\n",
        "    \n",
        "    axes[idx].scatter(X_moons[:, 0], X_moons[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "    noise = X_moons[labels == -1]\n",
        "    axes[idx].scatter(noise[:, 0], noise[:, 1], c='red', marker='x', s=100, linewidths=2)\n",
        "    axes[idx].set_title(f'ε = {eps}', fontweight='bold')\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise = list(labels).count(-1)\n",
        "    axes[idx].set_xlabel(f'{n_clusters} clusters, {n_noise} noise')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Observations:\")\n",
        "print(\"• Too small ε: Many noise points, fragmented clusters\")\n",
        "print(\"• Optimal ε: Clean separation, few noise points\")\n",
        "print(\"• Too large ε: Clusters merge together\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.6 DBSCAN from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DBSCANFromScratch:\n",
        "    def __init__(self, eps=0.5, min_samples=5):\n",
        "        self.eps = eps\n",
        "        self.min_samples = min_samples\n",
        "        self.labels_ = None\n",
        "    \n",
        "    def fit_predict(self, X):\n",
        "        n_samples = len(X)\n",
        "        labels = np.full(n_samples, -1)  # -1 means noise\n",
        "        cluster_id = 0\n",
        "        \n",
        "        # Compute distance matrix\n",
        "        from scipy.spatial.distance import cdist\n",
        "        distances = cdist(X, X, metric='euclidean')\n",
        "        \n",
        "        # Find neighbors for each point\n",
        "        neighbors = [np.where(distances[i] <= self.eps)[0] for i in range(n_samples)]\n",
        "        \n",
        "        # Identify core points\n",
        "        core_points = [i for i in range(n_samples) if len(neighbors[i]) >= self.min_samples]\n",
        "        \n",
        "        # Expand clusters from core points\n",
        "        for point in core_points:\n",
        "            if labels[point] != -1:\n",
        "                continue  # Already assigned\n",
        "            \n",
        "            # Start new cluster\n",
        "            labels[point] = cluster_id\n",
        "            seeds = list(neighbors[point])\n",
        "            \n",
        "            # Expand cluster\n",
        "            i = 0\n",
        "            while i < len(seeds):\n",
        "                q = seeds[i]\n",
        "                \n",
        "                if labels[q] == -1:\n",
        "                    labels[q] = cluster_id\n",
        "                \n",
        "                if labels[q] != -1:\n",
        "                    i += 1\n",
        "                    continue\n",
        "                \n",
        "                labels[q] = cluster_id\n",
        "                \n",
        "                # If q is core point, add its neighbors\n",
        "                if len(neighbors[q]) >= self.min_samples:\n",
        "                    seeds.extend(neighbors[q])\n",
        "                \n",
        "                i += 1\n",
        "            \n",
        "            cluster_id += 1\n",
        "        \n",
        "        self.labels_ = labels\n",
        "        return labels\n",
        "\n",
        "# Test implementation\n",
        "dbscan_scratch = DBSCANFromScratch(eps=0.3, min_samples=5)\n",
        "labels_scratch = dbscan_scratch.fit_predict(X_moons)\n",
        "\n",
        "dbscan_sklearn = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels_sklearn = dbscan_sklearn.fit_predict(X_moons)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_scratch, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[0].set_title('From Scratch', fontweight='bold')\n",
        "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_sklearn, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[1].set_title('Scikit-learn', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"From-scratch DBSCAN validated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 10: Spectral Clustering\n",
        "\n",
        "Spectral clustering uses graph theory and eigenvalues to find clusters.\n",
        "\n",
        "## 10.1 Core Concepts\n",
        "\n",
        "**Algorithm:**\n",
        "1. Construct similarity graph\n",
        "2. Compute graph Laplacian\n",
        "3. Find eigenvectors\n",
        "4. Apply K-Means on eigenvectors\n",
        "\n",
        "**Advantages:**\n",
        "- Finds non-convex clusters\n",
        "- Works well with graph-structured data\n",
        "- Can capture complex cluster shapes\n",
        "\n",
        "**Disadvantages:**\n",
        "- Computationally expensive\n",
        "- Requires choosing number of clusters\n",
        "- Sensitive to similarity metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Spectral vs K-Means on concentric circles\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Original\n",
        "axes[0].scatter(X_circles[:, 0], X_circles[:, 1], s=50, alpha=0.6)\n",
        "axes[0].set_title('Original Data (Concentric Circles)', fontweight='bold')\n",
        "\n",
        "# K-Means\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "labels_km = kmeans.fit_predict(X_circles)\n",
        "axes[1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_km, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[1].set_title('K-Means (Fails)', fontweight='bold')\n",
        "\n",
        "# Spectral\n",
        "spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=42)\n",
        "labels_sp = spectral.fit_predict(X_circles)\n",
        "axes[2].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_sp, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[2].set_title('Spectral Clustering (Succeeds)', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Spectral clustering successfully separates concentric circles!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.2 Mathematical Foundation\n",
        "\n",
        "Spectral clustering uses eigenvalues and eigenvectors of the graph Laplacian.\n",
        "\n",
        "**Steps:**\n",
        "1. **Construct affinity matrix W**: Similarity between points\n",
        "2. **Compute degree matrix D**: D_ii = Σ W_ij\n",
        "3. **Compute Laplacian**: L = D - W (unnormalized) or L_norm = D^(-1/2) L D^(-1/2) (normalized)\n",
        "4. **Find eigenvectors**: Compute k smallest eigenvectors of L\n",
        "5. **Apply K-Means**: Cluster rows of eigenvector matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate spectral clustering step-by-step\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from scipy.linalg import eigh\n",
        "\n",
        "# Use circles dataset\n",
        "X_demo = X_circles[:50]  # Smaller for visualization\n",
        "\n",
        "# Step 1: Affinity matrix\n",
        "gamma = 1.0\n",
        "W = rbf_kernel(X_demo, gamma=gamma)\n",
        "\n",
        "# Step 2: Degree matrix\n",
        "D = np.diag(W.sum(axis=1))\n",
        "\n",
        "# Step 3: Normalized Laplacian\n",
        "D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D)))\n",
        "L_norm = np.eye(len(X_demo)) - D_inv_sqrt @ W @ D_inv_sqrt\n",
        "\n",
        "# Step 4: Eigendecomposition\n",
        "eigenvalues, eigenvectors = eigh(L_norm)\n",
        "\n",
        "# Visualize the process\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Original data\n",
        "axes[0, 0].scatter(X_demo[:, 0], X_demo[:, 1], s=100, alpha=0.6, edgecolors='k')\n",
        "axes[0, 0].set_title('1. Original Data', fontweight='bold')\n",
        "\n",
        "# Affinity matrix\n",
        "im1 = axes[0, 1].imshow(W, cmap='viridis')\n",
        "axes[0, 1].set_title('2. Affinity Matrix W', fontweight='bold')\n",
        "plt.colorbar(im1, ax=axes[0, 1])\n",
        "\n",
        "# Laplacian\n",
        "im2 = axes[0, 2].imshow(L_norm, cmap='RdBu')\n",
        "axes[0, 2].set_title('3. Normalized Laplacian', fontweight='bold')\n",
        "plt.colorbar(im2, ax=axes[0, 2])\n",
        "\n",
        "# Eigenvalue spectrum\n",
        "axes[1, 0].plot(eigenvalues[:20], 'bo-', linewidth=2, markersize=8)\n",
        "axes[1, 0].set_title('4. Eigenvalue Spectrum', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Index')\n",
        "axes[1, 0].set_ylabel('Eigenvalue')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].axvline(x=2, color='r', linestyle='--', label='k=2')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Eigenvector space\n",
        "X_embedded = eigenvectors[:, :2]\n",
        "axes[1, 1].scatter(X_embedded[:, 0], X_embedded[:, 1], s=100, alpha=0.6, edgecolors='k')\n",
        "axes[1, 1].set_title('5. Eigenvector Space', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('1st eigenvector')\n",
        "axes[1, 1].set_ylabel('2nd eigenvector')\n",
        "\n",
        "# Final clustering\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans_spectral = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "labels_spectral = kmeans_spectral.fit_predict(X_embedded)\n",
        "axes[1, 2].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_spectral, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\n",
        "axes[1, 2].set_title('6. Final Clusters', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Spectral clustering transforms the data into eigenvector space where K-Means works!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.3 Eigengap Heuristic\n",
        "\n",
        "The **eigengap** helps choose the number of clusters k.\n",
        "\n",
        "**Rule:** Choose k where there's a large gap between λ_k and λ_(k+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eigengap heuristic demonstration\n",
        "W_full = rbf_kernel(X_circles, gamma=1.0)\n",
        "D_full = np.diag(W_full.sum(axis=1))\n",
        "D_inv_sqrt_full = np.diag(1.0 / np.sqrt(np.diag(D_full)))\n",
        "L_norm_full = np.eye(len(X_circles)) - D_inv_sqrt_full @ W_full @ D_inv_sqrt_full\n",
        "eigenvalues_full, _ = eigh(L_norm_full)\n",
        "\n",
        "# Plot eigenvalues and eigengaps\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Eigenvalue spectrum\n",
        "axes[0].plot(eigenvalues_full[:15], 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].set_title('Eigenvalue Spectrum', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Index')\n",
        "axes[0].set_ylabel('Eigenvalue')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Eigengap\n",
        "eigengaps = np.diff(eigenvalues_full[:15])\n",
        "axes[1].bar(range(1, len(eigengaps)+1), eigengaps)\n",
        "axes[1].set_title('Eigengap (λ_k - λ_(k-1))', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('k')\n",
        "axes[1].set_ylabel('Gap')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Highlight largest gap\n",
        "max_gap_idx = np.argmax(eigengaps) + 1\n",
        "axes[1].bar(max_gap_idx, eigengaps[max_gap_idx-1], color='red', label=f'Largest gap at k={max_gap_idx}')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Eigengap heuristic suggests k = {max_gap_idx}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.4 Affinity Matrix Construction\n",
        "\n",
        "Different ways to build the affinity matrix:\n",
        "1. **RBF kernel**: W_ij = exp(-γ||x_i - x_j||²)\n",
        "2. **K-nearest neighbors**: Connect only k nearest neighbors\n",
        "3. **ε-neighborhood**: Connect if distance < ε"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare affinity types\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "\n",
        "# RBF kernel\n",
        "spectral_rbf = SpectralClustering(n_clusters=2, affinity='rbf', gamma=1.0, random_state=42)\n",
        "labels_rbf = spectral_rbf.fit_predict(X_circles)\n",
        "axes[0, 0].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_rbf, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[0, 0].set_title('RBF Kernel', fontweight='bold')\n",
        "\n",
        "# Nearest neighbors\n",
        "spectral_nn = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', n_neighbors=10, random_state=42)\n",
        "labels_nn = spectral_nn.fit_predict(X_circles)\n",
        "axes[0, 1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_nn, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[0, 1].set_title('Nearest Neighbors (k=10)', fontweight='bold')\n",
        "\n",
        "# Different gamma for RBF\n",
        "spectral_rbf2 = SpectralClustering(n_clusters=2, affinity='rbf', gamma=0.1, random_state=42)\n",
        "labels_rbf2 = spectral_rbf2.fit_predict(X_circles)\n",
        "axes[1, 0].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_rbf2, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[1, 0].set_title('RBF Kernel (γ=0.1)', fontweight='bold')\n",
        "\n",
        "# Different k for NN\n",
        "spectral_nn2 = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', n_neighbors=5, random_state=42)\n",
        "labels_nn2 = spectral_nn2.fit_predict(X_circles)\n",
        "axes[1, 1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_nn2, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[1, 1].set_title('Nearest Neighbors (k=5)', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Affinity matrix construction affects clustering results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.5 Spectral Clustering from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpectralClusteringFromScratch:\n",
        "    def __init__(self, n_clusters=2, gamma=1.0):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.gamma = gamma\n",
        "        self.labels_ = None\n",
        "    \n",
        "    def fit_predict(self, X):\n",
        "        # Step 1: Compute affinity matrix (RBF kernel)\n",
        "        W = rbf_kernel(X, gamma=self.gamma)\n",
        "        \n",
        "        # Step 2: Compute degree matrix\n",
        "        D = np.diag(W.sum(axis=1))\n",
        "        \n",
        "        # Step 3: Normalized Laplacian\n",
        "        D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))  # Add small value for stability\n",
        "        L_norm = np.eye(len(X)) - D_inv_sqrt @ W @ D_inv_sqrt\n",
        "        \n",
        "        # Step 4: Eigendecomposition\n",
        "        eigenvalues, eigenvectors = eigh(L_norm)\n",
        "        \n",
        "        # Step 5: Select k smallest eigenvectors\n",
        "        X_embedded = eigenvectors[:, :self.n_clusters]\n",
        "        \n",
        "        # Step 6: Normalize rows\n",
        "        X_embedded = X_embedded / (np.linalg.norm(X_embedded, axis=1, keepdims=True) + 1e-10)\n",
        "        \n",
        "        # Step 7: Apply K-Means\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
        "        self.labels_ = kmeans.fit_predict(X_embedded)\n",
        "        \n",
        "        return self.labels_\n",
        "\n",
        "# Test implementation\n",
        "spectral_scratch = SpectralClusteringFromScratch(n_clusters=2, gamma=1.0)\n",
        "labels_scratch = spectral_scratch.fit_predict(X_circles)\n",
        "\n",
        "spectral_sklearn = SpectralClustering(n_clusters=2, affinity='rbf', gamma=1.0, random_state=42)\n",
        "labels_sklearn = spectral_sklearn.fit_predict(X_circles)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_scratch, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[0].set_title('From Scratch', fontweight='bold')\n",
        "axes[1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_sklearn, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[1].set_title('Scikit-learn', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"From-scratch Spectral Clustering validated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 11: Cluster Quality Evaluation\n",
        "\n",
        "How do we measure clustering quality?\n",
        "\n",
        "## 11.1 Silhouette Score (No Labels Required)\n",
        "\n",
        "Measures how similar a point is to its own cluster compared to other clusters.\n",
        "\n",
        "**Formula:** s(i) = (b(i) - a(i)) / max{a(i), b(i)}\n",
        "- a(i): Mean distance to points in same cluster\n",
        "- b(i): Mean distance to points in nearest cluster\n",
        "- Range: [-1, +1], higher is better\n",
        "- +1: Point is well-clustered\n",
        "- 0: Point is on cluster boundary\n",
        "- -1: Point is in wrong cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate clustering quality on Iris\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_true = iris.target\n",
        "\n",
        "# Try different k values\n",
        "k_values = range(2, 7)\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_iris)\n",
        "    score = silhouette_score(X_iris, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_values, silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
        "plt.ylabel('Silhouette Score', fontsize=12)\n",
        "plt.title('Silhouette Score vs Number of Clusters', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_k = k_values[np.argmax(silhouette_scores)]\n",
        "print(f\"Best k based on Silhouette Score: {best_k}\")\n",
        "print(f\"Silhouette scores: {dict(zip(k_values, [f'{s:.3f}' for s in silhouette_scores]))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.2 Purity (Requires Ground Truth)\n",
        "\n",
        "**Purity** measures how homogeneous clusters are.\n",
        "\n",
        "**Formula:** Purity = (1/N) Σ max_j |C_i ∩ L_j|\n",
        "\n",
        "- Range: [0, 1], higher is better\n",
        "- Perfect clustering: purity = 1\n",
        "- Problem: Trivial solution (each point its own cluster) gives purity = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def purity_score(y_true, y_pred):\n",
        "    # Compute confusion matrix\n",
        "    contingency_matrix = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))))\n",
        "    for i, true_label in enumerate(np.unique(y_true)):\n",
        "        for j, pred_label in enumerate(np.unique(y_pred)):\n",
        "            contingency_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
        "    \n",
        "    # Purity is sum of max values in each column divided by total\n",
        "    return np.sum(np.max(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
        "\n",
        "# Test on Iris\n",
        "kmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "labels_iris = kmeans_iris.fit_predict(X_iris)\n",
        "\n",
        "purity = purity_score(y_true, labels_iris)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# True labels\n",
        "axes[0].scatter(X_iris[:, 0], X_iris[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[0].set_title('True Labels', fontweight='bold')\n",
        "axes[0].set_xlabel('Sepal Length')\n",
        "axes[0].set_ylabel('Sepal Width')\n",
        "\n",
        "# Predicted clusters\n",
        "axes[1].scatter(X_iris[:, 0], X_iris[:, 1], c=labels_iris, cmap='viridis', s=50, alpha=0.6, edgecolors='k')\n",
        "axes[1].set_title(f'K-Means Clusters (Purity={purity:.3f})', fontweight='bold')\n",
        "axes[1].set_xlabel('Sepal Length')\n",
        "axes[1].set_ylabel('Sepal Width')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Purity Score: {purity:.3f}\")\n",
        "print(\"Higher purity means clusters are more homogeneous\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.3 Entropy (Requires Ground Truth)\n",
        "\n",
        "**Entropy** measures the disorder within clusters.\n",
        "\n",
        "**Formula:** Entropy = Σ (N_i/N) × e(C_i)\n",
        "\n",
        "where e(C_i) = -Σ p_ij log(p_ij)\n",
        "\n",
        "- Range: [0, log(k)], lower is better\n",
        "- Perfect clustering: entropy = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy_score(y_true, y_pred):\n",
        "    from scipy.stats import entropy as scipy_entropy\n",
        "    \n",
        "    total_entropy = 0\n",
        "    n_samples = len(y_true)\n",
        "    \n",
        "    for cluster_label in np.unique(y_pred):\n",
        "        cluster_mask = y_pred == cluster_label\n",
        "        cluster_size = np.sum(cluster_mask)\n",
        "        \n",
        "        if cluster_size == 0:\n",
        "            continue\n",
        "        \n",
        "        # Distribution of true labels in this cluster\n",
        "        label_counts = np.bincount(y_true[cluster_mask], minlength=len(np.unique(y_true)))\n",
        "        label_probs = label_counts / cluster_size\n",
        "        \n",
        "        # Entropy of this cluster\n",
        "        cluster_entropy = scipy_entropy(label_probs, base=2)\n",
        "        \n",
        "        # Weighted by cluster size\n",
        "        total_entropy += (cluster_size / n_samples) * cluster_entropy\n",
        "    \n",
        "    return total_entropy\n",
        "\n",
        "entropy = entropy_score(y_true, labels_iris)\n",
        "print(f\"Entropy Score: {entropy:.3f}\")\n",
        "print(f\"Purity Score: {purity:.3f}\")\n",
        "print(\"\\nLower entropy = better clustering\")\n",
        "print(\"Higher purity = better clustering\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.4 RAND Index (Requires Ground Truth)\n",
        "\n",
        "**RAND Index** measures agreement between two clusterings.\n",
        "\n",
        "**Formula:** RAND = (A + B) / (A + B + C + D)\n",
        "\n",
        "where:\n",
        "- A: Pairs in same cluster in both\n",
        "- B: Pairs in different clusters in both\n",
        "- C: Pairs in same cluster in predicted, different in true\n",
        "- D: Pairs in different clusters in predicted, same in true\n",
        "\n",
        "**Adjusted RAND Index** corrects for chance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import rand_score, adjusted_rand_score\n",
        "\n",
        "rand = rand_score(y_true, labels_iris)\n",
        "adj_rand = adjusted_rand_score(y_true, labels_iris)\n",
        "\n",
        "print(f\"RAND Index: {rand:.3f}\")\n",
        "print(f\"Adjusted RAND Index: {adj_rand:.3f}\")\n",
        "print(\"\\nAdjusted RAND Index:\")\n",
        "print(\"• Range: [-1, 1]\")\n",
        "print(\"• 1: Perfect match\")\n",
        "print(\"• 0: Random labeling\")\n",
        "print(\"• Negative: Worse than random\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5 Silhouette Score (No Ground Truth Required)\n",
        "\n",
        "We already covered this, but let's add silhouette plot visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "# Compute silhouette for each sample\n",
        "silhouette_vals = silhouette_samples(X_iris, labels_iris)\n",
        "silhouette_avg = np.mean(silhouette_vals)\n",
        "\n",
        "# Create silhouette plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Silhouette plot\n",
        "y_lower = 10\n",
        "for i in range(3):\n",
        "    cluster_silhouette_vals = silhouette_vals[labels_iris == i]\n",
        "    cluster_silhouette_vals.sort()\n",
        "    \n",
        "    size_cluster_i = cluster_silhouette_vals.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    \n",
        "    color = plt.cm.viridis(float(i) / 3)\n",
        "    axes[0].fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "    \n",
        "    axes[0].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "axes[0].set_title('Silhouette Plot', fontweight='bold')\n",
        "axes[0].set_xlabel('Silhouette Coefficient')\n",
        "axes[0].set_ylabel('Cluster')\n",
        "axes[0].axvline(x=silhouette_avg, color='red', linestyle='--', label=f'Average: {silhouette_avg:.3f}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Scatter plot colored by silhouette score\n",
        "scatter = axes[1].scatter(X_iris[:, 0], X_iris[:, 1], c=silhouette_vals, cmap='RdYlGn', \n",
        "                         s=50, alpha=0.6, edgecolors='k', vmin=-0.2, vmax=1)\n",
        "axes[1].set_title('Points Colored by Silhouette Score', fontweight='bold')\n",
        "axes[1].set_xlabel('Sepal Length')\n",
        "axes[1].set_ylabel('Sepal Width')\n",
        "plt.colorbar(scatter, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average Silhouette Score: {silhouette_avg:.3f}\")\n",
        "print(\"Green points: Well-clustered\")\n",
        "print(\"Yellow/Red points: Poorly clustered or misassigned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.6 Comprehensive Comparison\n",
        "\n",
        "Let's compare all metrics on Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute all metrics\n",
        "from sklearn.metrics import adjusted_mutual_info_score, fowlkes_mallows_score\n",
        "\n",
        "metrics = {\n",
        "    'Purity': purity,\n",
        "    'Entropy': entropy,\n",
        "    'RAND Index': rand,\n",
        "    'Adjusted RAND': adj_rand,\n",
        "    'Silhouette': silhouette_avg,\n",
        "    'Adj. Mutual Info': adjusted_mutual_info_score(y_true, labels_iris),\n",
        "    'Fowlkes-Mallows': fowlkes_mallows_score(y_true, labels_iris)\n",
        "}\n",
        "\n",
        "# Display as table\n",
        "print(\"\\nCluster Quality Metrics for K-Means on Iris:\")\n",
        "print(\"=\" * 50)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric:20s}: {value:6.3f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"• Purity, RAND, Silhouette, AMI, FM: Higher is better\")\n",
        "print(\"• Entropy: Lower is better\")\n",
        "print(\"• Adjusted metrics correct for chance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.7 Summary: Choosing the Right Metric\n",
        "\n",
        "| Metric | Requires Labels | Range | Best Value | Use When |\n",
        "|--------|----------------|-------|------------|----------|\n",
        "| **Purity** | Yes | [0, 1] | 1 | Simple interpretation |\n",
        "| **Entropy** | Yes | [0, log k] | 0 | Information-theoretic view |\n",
        "| **RAND Index** | Yes | [0, 1] | 1 | Pair-wise agreement |\n",
        "| **Adj. RAND** | Yes | [-1, 1] | 1 | Corrects for chance |\n",
        "| **Silhouette** | No | [-1, 1] | 1 | No ground truth available |\n",
        "| **Adj. MI** | Yes | [0, 1] | 1 | Information-based, corrected |\n",
        "\n",
        "**Recommendations:**\n",
        "- **With ground truth**: Use Adjusted RAND or Adjusted Mutual Information\n",
        "- **Without ground truth**: Use Silhouette Score\n",
        "- **For interpretation**: Use Purity (easy to understand)\n",
        "- **For research**: Report multiple metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 12: Algorithm Comparison Summary\n",
        "\n",
        "| Algorithm | Cluster Shape | Requires k | Handles Noise | Time Complexity | Best For |\n",
        "|-----------|---------------|------------|---------------|-----------------|----------|\n",
        "| **K-Means** | Spherical | Yes | No | O(nkt) | Large datasets, spherical clusters |\n",
        "| **Hierarchical** | Any | No | No | O(n²log n) | Small datasets, hierarchy needed |\n",
        "| **DBSCAN** | Arbitrary | No | Yes | O(n log n) | Arbitrary shapes, noise detection |\n",
        "| **Spectral** | Non-convex | Yes | No | O(n³) | Graph data, complex shapes |\n",
        "\n",
        "## Decision Guide\n",
        "\n",
        "**Use K-Means when:**\n",
        "- Clusters are roughly spherical\n",
        "- You know k in advance\n",
        "- You have large datasets\n",
        "- Speed is important\n",
        "\n",
        "**Use Hierarchical when:**\n",
        "- You need a hierarchy of clusters\n",
        "- You don't know k\n",
        "- Dataset is small (< 10,000 points)\n",
        "- You want to explore different k values\n",
        "\n",
        "**Use DBSCAN when:**\n",
        "- Clusters have arbitrary shapes\n",
        "- You need to identify outliers\n",
        "- Cluster density is roughly uniform\n",
        "- You don't know k\n",
        "\n",
        "**Use Spectral when:**\n",
        "- Data has graph structure\n",
        "- Clusters are non-convex\n",
        "- You can afford computational cost\n",
        "- K-Means fails on your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "1. **No single best algorithm** - choice depends on data characteristics\n",
        "2. **K-Means** is fast and simple but assumes spherical clusters\n",
        "3. **Hierarchical** provides flexibility but is computationally expensive\n",
        "4. **DBSCAN** handles arbitrary shapes and noise but sensitive to parameters\n",
        "5. **Spectral** works on complex shapes but requires eigendecomposition\n",
        "6. **Always visualize** your data and results\n",
        "7. **Use multiple metrics** to evaluate clustering quality\n",
        "\n",
        "**Further Reading:**\n",
        "- Scikit-learn clustering documentation\n",
        "- \"Pattern Recognition and Machine Learning\" by Bishop\n",
        "- \"The Elements of Statistical Learning\" by Hastie et al."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
