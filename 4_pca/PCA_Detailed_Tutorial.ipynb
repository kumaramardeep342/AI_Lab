{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) - Comprehensive Tutorial\n",
    "\n",
    "**Instructor:** Dr. Arun B Ayyar  \n",
    "**Institution:** IIT Madras  \n",
    "**Course:** DA5400W - Foundations of Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction to Dimensionality Reduction\n",
    "2. Mathematical Foundation of PCA\n",
    "3. Step-by-Step PCA Algorithm\n",
    "4. Geometric Interpretation\n",
    "5. PCA from Scratch Implementation\n",
    "6. PCA with Scikit-learn\n",
    "7. Choosing the Number of Components\n",
    "8. Applications and Use Cases\n",
    "9. Limitations and Considerations\n",
    "10. Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to Dimensionality Reduction\n",
    "\n",
    "### What is Dimensionality Reduction?\n",
    "\n",
    "**Dimensionality reduction** is the process of reducing the number of features (dimensions) in a dataset while preserving as much information as possible.\n",
    "\n",
    "### Why Do We Need It?\n",
    "\n",
    "1. **Curse of Dimensionality**: As the number of features increases, the amount of data needed to maintain statistical significance grows exponentially\n",
    "2. **Visualization**: Humans can only visualize 2D or 3D data effectively\n",
    "3. **Computational Efficiency**: Fewer dimensions mean faster training and prediction\n",
    "4. **Noise Reduction**: Removing less important features can improve model performance\n",
    "5. **Feature Extraction**: Discover underlying patterns in high-dimensional data\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "**Principal Component Analysis (PCA)** is an unsupervised linear dimensionality reduction technique that:\n",
    "- Finds new axes (principal components) that maximize variance\n",
    "- Projects data onto these new axes\n",
    "- Reduces dimensionality by keeping only the top k components\n",
    "\n",
    "**Key Properties:**\n",
    "- Principal components are **orthogonal** (perpendicular) to each other\n",
    "- First component captures the **most variance**, second captures the second most, etc.\n",
    "- Components are **linear combinations** of original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: High-dimensional data problem\n",
    "# Create a dataset with 100 features\n",
    "X_high_dim, y = make_classification(n_samples=1000, n_features=100, \n",
    "                                     n_informative=10, n_redundant=80,\n",
    "                                     n_repeated=10, random_state=42)\n",
    "\n",
    "print(f\"Original data shape: {X_high_dim.shape}\")\n",
    "print(f\"Number of samples: {X_high_dim.shape[0]}\")\n",
    "print(f\"Number of features: {X_high_dim.shape[1]}\")\n",
    "print(f\"\\nChallenge: How do we visualize or understand 100-dimensional data?\")\n",
    "print(\"Solution: Use PCA to reduce to 2 or 3 dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Mathematical Foundation of PCA\n",
    "\n",
    "### Variance and Covariance\n",
    "\n",
    "**Variance** measures the spread of a single variable:\n",
    "\n",
    "$$\\text{Var}(X) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n",
    "\n",
    "**Covariance** measures how two variables vary together:\n",
    "\n",
    "$$\\text{Cov}(X, Y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "**Covariance Matrix** for d features:\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_d) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_d) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_d, X_1) & \\text{Cov}(X_d, X_2) & \\cdots & \\text{Var}(X_d)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "For a square matrix $A$ and vector $v$:\n",
    "\n",
    "$$Av = \\lambda v$$\n",
    "\n",
    "Where:\n",
    "- $v$ is an **eigenvector** (direction that doesn't change under transformation)\n",
    "- $\\lambda$ is an **eigenvalue** (scaling factor in that direction)\n",
    "\n",
    "**Key Insight:** The eigenvectors of the covariance matrix are the principal components, and the eigenvalues represent the variance explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Covariance and correlation\n",
    "# Create correlated 2D data\n",
    "np.random.seed(42)\n",
    "mean = [0, 0]\n",
    "cov = [[3, 2.5],   # Covariance matrix: variance of X=3, covariance=2.5\n",
    "       [2.5, 3]]   # variance of Y=3\n",
    "data_2d = np.random.multivariate_normal(mean, cov, 500)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = np.cov(data_2d.T)\n",
    "\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "print(f\"\\nVariance of X: {cov_matrix[0, 0]:.2f}\")\n",
    "print(f\"Variance of Y: {cov_matrix[1, 1]:.2f}\")\n",
    "print(f\"Covariance(X, Y): {cov_matrix[0, 1]:.2f}\")\n",
    "print(f\"\\nPositive covariance means X and Y tend to increase together\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=30)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('Y', fontsize=12)\n",
    "axes[0].set_title('2D Data with Positive Covariance', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Covariance matrix heatmap\n",
    "sns.heatmap(cov_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            xticklabels=['X', 'Y'], yticklabels=['X', 'Y'], ax=axes[1],\n",
    "            cbar_kws={'label': 'Covariance'})\n",
    "axes[1].set_title('Covariance Matrix', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Eigenvectors and eigenvalues\n",
    "# Compute eigenvectors and eigenvalues of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(\"Eigenvalues (variance explained by each component):\")\n",
    "for i, eigenval in enumerate(eigenvalues):\n",
    "    print(f\"  PC{i+1}: {eigenval:.3f}\")\n",
    "\n",
    "print(\"\\nEigenvectors (principal component directions):\")\n",
    "for i, eigenvec in enumerate(eigenvectors.T):\n",
    "    print(f\"  PC{i+1}: [{eigenvec[0]:.3f}, {eigenvec[1]:.3f}]\")\n",
    "\n",
    "# Visualize eigenvectors on the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.5, s=30, label='Data')\n",
    "\n",
    "# Plot eigenvectors scaled by eigenvalues\n",
    "origin = np.mean(data_2d, axis=0)\n",
    "for i, (eigenval, eigenvec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    # Scale eigenvector by square root of eigenvalue (standard deviation)\n",
    "    plt.arrow(origin[0], origin[1], \n",
    "              eigenvec[0] * np.sqrt(eigenval) * 2, \n",
    "              eigenvec[1] * np.sqrt(eigenval) * 2,\n",
    "              head_width=0.3, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "              linewidth=3, label=f'PC{i+1} (Œª={eigenval:.2f})')\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('Y', fontsize=12)\n",
    "plt.title('Principal Components as Eigenvectors\\n(Arrow length = 2√ó‚àöeigenvalue)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. PC1 (red) points in the direction of maximum variance\")\n",
    "print(\"2. PC2 (orange) is perpendicular to PC1\")\n",
    "print(\"3. Eigenvalue of PC1 > Eigenvalue of PC2 (captures more variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Step-by-Step PCA Algorithm\n",
    "\n",
    "### The PCA Algorithm\n",
    "\n",
    "**Input:** Data matrix $X$ of shape $(n, d)$ where $n$ = samples, $d$ = features\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Standardize the data** (mean = 0, std = 1 for each feature)\n",
    "   $$X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "2. **Compute the covariance matrix**\n",
    "   $$\\Sigma = \\frac{1}{n-1}X_{\\text{std}}^T X_{\\text{std}}$$\n",
    "\n",
    "3. **Compute eigenvalues and eigenvectors** of $\\Sigma$\n",
    "   $$\\Sigma v_i = \\lambda_i v_i$$\n",
    "\n",
    "4. **Sort eigenvectors** by eigenvalues in descending order\n",
    "\n",
    "5. **Select top k eigenvectors** to form projection matrix $W$\n",
    "\n",
    "6. **Transform the data**\n",
    "   $$X_{\\text{PCA}} = X_{\\text{std}} \\cdot W$$\n",
    "\n",
    "**Output:** Reduced data matrix of shape $(n, k)$\n",
    "\n",
    "### Why Standardization?\n",
    "\n",
    "Features with larger scales dominate the variance. Standardization ensures all features contribute equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step PCA implementation\n",
    "# Using the 2D data from before\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP-BY-STEP PCA IMPLEMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "print(\"\\nStep 1: Standardize the data\")\n",
    "print(\"-\" * 40)\n",
    "mean = np.mean(data_2d, axis=0)\n",
    "std = np.std(data_2d, axis=0)\n",
    "data_standardized = (data_2d - mean) / std\n",
    "\n",
    "print(f\"Original mean: {mean}\")\n",
    "print(f\"Original std: {std}\")\n",
    "print(f\"Standardized mean: {np.mean(data_standardized, axis=0)}\")\n",
    "print(f\"Standardized std: {np.std(data_standardized, axis=0)}\")\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "print(\"\\nStep 2: Compute covariance matrix\")\n",
    "print(\"-\" * 40)\n",
    "cov_matrix = np.cov(data_standardized.T)\n",
    "print(\"Covariance matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Step 3: Compute eigenvalues and eigenvectors\n",
    "print(\"\\nStep 3: Compute eigenvalues and eigenvectors\")\n",
    "print(\"-\" * 40)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(\"Eigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Step 4: Sort by eigenvalues\n",
    "print(\"\\nStep 4: Sort eigenvectors by eigenvalues (descending)\")\n",
    "print(\"-\" * 40)\n",
    "idx = eigenvalues.argsort()[::-1]  # Descending order\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "print(f\"Sorted eigenvalues: {eigenvalues}\")\n",
    "print(\"Sorted eigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Step 5: Select top k components (k=1 for 2D to 1D reduction)\n",
    "print(\"\\nStep 5: Select top k=1 component\")\n",
    "print(\"-\" * 40)\n",
    "k = 1\n",
    "W = eigenvectors[:, :k]  # Projection matrix\n",
    "print(f\"Projection matrix W (shape {W.shape}):\")\n",
    "print(W)\n",
    "\n",
    "# Step 6: Transform the data\n",
    "print(\"\\nStep 6: Transform the data\")\n",
    "print(\"-\" * 40)\n",
    "data_pca = data_standardized @ W\n",
    "print(f\"Original data shape: {data_2d.shape}\")\n",
    "print(f\"Transformed data shape: {data_pca.shape}\")\n",
    "print(f\"\\nFirst 5 transformed samples:\")\n",
    "print(data_pca[:5])\n",
    "\n",
    "# Calculate variance explained\n",
    "variance_explained = eigenvalues / np.sum(eigenvalues) * 100\n",
    "print(f\"\\nVariance explained by PC1: {variance_explained[0]:.2f}%\")\n",
    "print(f\"Variance explained by PC2: {variance_explained[1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Geometric Interpretation\n",
    "\n",
    "### What Does PCA Do Geometrically?\n",
    "\n",
    "1. **Rotation**: PCA rotates the coordinate system to align with the directions of maximum variance\n",
    "2. **Projection**: Data is projected onto the new axes (principal components)\n",
    "3. **Dimensionality Reduction**: Keep only the top k components, discarding the rest\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine you have a 3D cloud of points shaped like a flat pancake. PCA finds:\n",
    "- **PC1**: The longest axis of the pancake\n",
    "- **PC2**: The second longest axis (perpendicular to PC1)\n",
    "- **PC3**: The thickness of the pancake (very small)\n",
    "\n",
    "By keeping only PC1 and PC2, you capture most of the information while reducing from 3D to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Before and after PCA\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original 2D data\n",
    "axes[0].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=30)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[0].set_xlabel('Original X', fontsize=12)\n",
    "axes[0].set_ylabel('Original Y', fontsize=12)\n",
    "axes[0].set_title('Original 2D Data', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# Data with principal components overlaid\n",
    "axes[1].scatter(data_standardized[:, 0], data_standardized[:, 1], alpha=0.6, s=30)\n",
    "origin = [0, 0]\n",
    "for i, (eigenval, eigenvec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    axes[1].arrow(origin[0], origin[1], \n",
    "                  eigenvec[0] * np.sqrt(eigenval) * 2, \n",
    "                  eigenvec[1] * np.sqrt(eigenval) * 2,\n",
    "                  head_width=0.15, head_length=0.15, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "                  linewidth=3, label=f'PC{i+1}')\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[1].set_xlabel('Standardized X', fontsize=12)\n",
    "axes[1].set_ylabel('Standardized Y', fontsize=12)\n",
    "axes[1].set_title('Standardized Data + Principal Components', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "# Projected onto PC1 (1D)\n",
    "axes[2].scatter(data_pca, np.zeros_like(data_pca), alpha=0.6, s=30)\n",
    "axes[2].axhline(y=0, color='k', linewidth=1)\n",
    "axes[2].set_xlabel('PC1', fontsize=12)\n",
    "axes[2].set_ylabel('(Reduced to 1D)', fontsize=12)\n",
    "axes[2].set_title(f'After PCA: 2D ‚Üí 1D\\n({variance_explained[0]:.1f}% variance retained)', fontsize=14)\n",
    "axes[2].set_ylim(-0.5, 0.5)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGeometric Interpretation:\")\n",
    "print(\"1. Left: Original data with correlation between X and Y\")\n",
    "print(\"2. Middle: Principal components (red and orange arrows) show new axes\")\n",
    "print(\"3. Right: Data projected onto PC1 only (dimensionality reduced from 2D to 1D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: PCA from Scratch - Complete Implementation\n",
    "\n",
    "Let's implement a complete PCA class from scratch to solidify our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_FromScratch:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        \"\"\"\n",
    "        Initialize PCA.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of principal components to keep\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.eigenvalues = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA on the data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \"\"\"\n",
    "        # Step 1: Standardize\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        X_std = (X - self.mean) / self.std\n",
    "        \n",
    "        # Step 2: Covariance matrix\n",
    "        cov_matrix = np.cov(X_std.T)\n",
    "        \n",
    "        # Step 3: Eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # Step 4: Sort by eigenvalues\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Step 5: Select top k components\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "        self.eigenvalues = eigenvalues[:self.n_components]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to principal component space.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data to transform\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : array-like, shape (n_samples, n_components)\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        # Standardize using training statistics\n",
    "        X_std = (X - self.mean) / self.std\n",
    "        \n",
    "        # Project onto principal components\n",
    "        return X_std @ self.components\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def explained_variance_ratio(self):\n",
    "        \"\"\"\n",
    "        Return the proportion of variance explained by each component.\n",
    "        \"\"\"\n",
    "        total_variance = np.sum(self.eigenvalues)\n",
    "        return self.eigenvalues / total_variance\n",
    "    \n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"\n",
    "        Transform data back to original space.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_transformed : array-like, shape (n_samples, n_components)\n",
    "            Transformed data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_reconstructed : array-like, shape (n_samples, n_features)\n",
    "            Reconstructed data (approximation)\n",
    "        \"\"\"\n",
    "        X_std = X_transformed @ self.components.T\n",
    "        return X_std * self.std + self.mean\n",
    "\n",
    "print(\"PCA_FromScratch class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our implementation\n",
    "print(\"Testing PCA_FromScratch implementation\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create test data\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "\n",
    "# Apply our PCA\n",
    "pca_scratch = PCA_FromScratch(n_components=2)\n",
    "X_transformed = pca_scratch.fit_transform(X_test)\n",
    "\n",
    "print(f\"Original shape: {X_test.shape}\")\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")\n",
    "print(f\"\\nExplained variance ratio: {pca_scratch.explained_variance_ratio()}\")\n",
    "print(f\"Total variance explained: {np.sum(pca_scratch.explained_variance_ratio()):.2%}\")\n",
    "\n",
    "# Reconstruct data\n",
    "X_reconstructed = pca_scratch.inverse_transform(X_transformed)\n",
    "reconstruction_error = np.mean((X_test - X_reconstructed) ** 2)\n",
    "print(f\"\\nReconstruction error (MSE): {reconstruction_error:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì Implementation working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: PCA with Scikit-learn\n",
    "\n",
    "While understanding the mathematics is crucial, in practice we use optimized libraries like scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the famous Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Iris Dataset\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples: {X_iris.shape[0]}\")\n",
    "print(f\"Number of features: {X_iris.shape[1]}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"Target names: {target_names}\")\n",
    "print(f\"\\nFirst 5 samples:\")\n",
    "print(pd.DataFrame(X_iris[:5], columns=feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA using scikit-learn\n",
    "# Standardize first\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Apply PCA to reduce from 4D to 2D\n",
    "pca_sklearn = PCA(n_components=2)\n",
    "X_iris_pca = pca_sklearn.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(\"PCA with Scikit-learn\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original shape: {X_iris.shape}\")\n",
    "print(f\"Transformed shape: {X_iris_pca.shape}\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "for i, var in enumerate(pca_sklearn.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "print(f\"\\nTotal variance explained: {np.sum(pca_sklearn.explained_variance_ratio_):.4f} ({np.sum(pca_sklearn.explained_variance_ratio_)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nPrincipal components (loadings):\")\n",
    "components_df = pd.DataFrame(\n",
    "    pca_sklearn.components_,\n",
    "    columns=feature_names,\n",
    "    index=['PC1', 'PC2']\n",
    ")\n",
    "print(components_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Iris dataset in 2D after PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot colored by species\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, (target, name) in enumerate(zip(range(3), target_names)):\n",
    "    mask = y_iris == target\n",
    "    axes[0].scatter(X_iris_pca[mask, 0], X_iris_pca[mask, 1], \n",
    "                    c=colors[i], label=name, alpha=0.7, s=60, edgecolors='k', linewidth=0.5)\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca_sklearn.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_sklearn.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_title('Iris Dataset: PCA Projection (4D ‚Üí 2D)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Component loadings (biplot)\n",
    "for i, (target, name) in enumerate(zip(range(3), target_names)):\n",
    "    mask = y_iris == target\n",
    "    axes[1].scatter(X_iris_pca[mask, 0], X_iris_pca[mask, 1], \n",
    "                    c=colors[i], label=name, alpha=0.5, s=40)\n",
    "\n",
    "# Plot feature vectors\n",
    "for i, feature in enumerate(feature_names):\n",
    "    axes[1].arrow(0, 0, \n",
    "                  pca_sklearn.components_[0, i] * 3, \n",
    "                  pca_sklearn.components_[1, i] * 3,\n",
    "                  head_width=0.15, head_length=0.15, fc='black', ec='black', linewidth=2)\n",
    "    axes[1].text(pca_sklearn.components_[0, i] * 3.3, \n",
    "                 pca_sklearn.components_[1, i] * 3.3,\n",
    "                 feature.replace(' (cm)', ''), fontsize=10, fontweight='bold')\n",
    "\n",
    "axes[1].set_xlabel(f'PC1 ({pca_sklearn.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_ylabel(f'PC2 ({pca_sklearn.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_title('PCA Biplot: Data + Feature Loadings', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Left plot: Iris species are well-separated in the 2D PCA space\")\n",
    "print(\"- Right plot (biplot): Arrows show how original features contribute to PCs\")\n",
    "print(\"  * Longer arrow = stronger contribution\")\n",
    "print(\"  * Arrow direction = correlation with PCs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Choosing the Number of Components\n",
    "\n",
    "### Methods for Selecting k\n",
    "\n",
    "1. **Explained Variance Threshold**: Keep components until you reach a target (e.g., 95% variance)\n",
    "2. **Scree Plot**: Look for the \"elbow\" where eigenvalues drop off\n",
    "3. **Cumulative Variance Plot**: Visualize cumulative variance explained\n",
    "4. **Cross-Validation**: Use downstream task performance to select k\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "- **Visualization**: k = 2 or 3\n",
    "- **Dimensionality Reduction**: Keep 80-95% of variance\n",
    "- **Noise Reduction**: Keep components with eigenvalues > 1 (Kaiser criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_iris_scaled)\n",
    "\n",
    "# Create visualizations for choosing k\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Scree plot\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_) + 1), \n",
    "            pca_full.explained_variance_, alpha=0.7, color='steelblue')\n",
    "axes[0].plot(range(1, len(pca_full.explained_variance_) + 1), \n",
    "             pca_full.explained_variance_, 'ro-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Eigenvalue (Variance)', fontsize=12)\n",
    "axes[0].set_title('Scree Plot', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(1, len(pca_full.explained_variance_) + 1))\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add Kaiser criterion line (eigenvalue = 1)\n",
    "axes[0].axhline(y=1, color='red', linestyle='--', linewidth=2, label='Kaiser criterion (Œª=1)')\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# 2. Explained variance ratio\n",
    "axes[1].bar(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "            pca_full.explained_variance_ratio_ * 100, alpha=0.7, color='coral')\n",
    "axes[1].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[1].set_ylabel('Variance Explained (%)', fontsize=12)\n",
    "axes[1].set_title('Variance Explained by Each PC', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(1, len(pca_full.explained_variance_ratio_) + 1))\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Cumulative variance explained\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_) * 100\n",
    "axes[2].plot(range(1, len(cumsum) + 1), cumsum, 'go-', linewidth=3, markersize=10)\n",
    "axes[2].fill_between(range(1, len(cumsum) + 1), cumsum, alpha=0.3, color='green')\n",
    "axes[2].axhline(y=95, color='red', linestyle='--', linewidth=2, label='95% threshold')\n",
    "axes[2].axhline(y=90, color='orange', linestyle='--', linewidth=2, label='90% threshold')\n",
    "axes[2].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[2].set_ylabel('Cumulative Variance Explained (%)', fontsize=12)\n",
    "axes[2].set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(range(1, len(cumsum) + 1))\n",
    "axes[2].set_ylim([0, 105])\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed information\n",
    "print(\"\\nVariance Explained by Each Component:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (var, cumvar) in enumerate(zip(pca_full.explained_variance_ratio_, cumsum)):\n",
    "    print(f\"PC{i+1}: {var*100:.2f}% (Cumulative: {cumvar:.2f}%)\")\n",
    "\n",
    "# Recommendation\n",
    "n_components_90 = np.argmax(cumsum >= 90) + 1\n",
    "n_components_95 = np.argmax(cumsum >= 95) + 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Recommendations:\")\n",
    "print(f\"  ‚Ä¢ For 90% variance: Use {n_components_90} components\")\n",
    "print(f\"  ‚Ä¢ For 95% variance: Use {n_components_95} components\")\n",
    "print(f\"  ‚Ä¢ For visualization: Use 2 components ({cumsum[1]:.1f}% variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Applications and Use Cases\n",
    "\n",
    "### Common Applications of PCA\n",
    "\n",
    "1. **Visualization**: Reduce high-dimensional data to 2D/3D for plotting\n",
    "2. **Noise Reduction**: Remove components with low variance (likely noise)\n",
    "3. **Feature Extraction**: Create new features that capture most variance\n",
    "4. **Data Compression**: Reduce storage requirements\n",
    "5. **Preprocessing**: Improve machine learning model performance\n",
    "6. **Exploratory Data Analysis**: Understand data structure and relationships\n",
    "\n",
    "### Example: Handwritten Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset (8x8 images = 64 features)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(\"Handwritten Digits Dataset\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples: {X_digits.shape[0]}\")\n",
    "print(f\"Number of features (pixels): {X_digits.shape[1]}\")\n",
    "print(f\"Image shape: 8√ó8 pixels\")\n",
    "print(f\"Number of classes: {len(np.unique(y_digits))}\")\n",
    "\n",
    "# Display some sample digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_digits[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_digits[i]}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Handwritten Digits (8√ó8 = 64 pixels)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to digits\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "# Reduce from 64D to 2D for visualization\n",
    "pca_digits = PCA(n_components=2)\n",
    "X_digits_pca = pca_digits.fit_transform(X_digits_scaled)\n",
    "\n",
    "print(f\"Reduced from {X_digits.shape[1]}D to {X_digits_pca.shape[1]}D\")\n",
    "print(f\"Variance explained: {np.sum(pca_digits.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# Visualize in 2D\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(X_digits_pca[:, 0], X_digits_pca[:, 1], \n",
    "                      c=y_digits, cmap='tab10', alpha=0.7, s=30, edgecolors='k', linewidth=0.3)\n",
    "plt.colorbar(scatter, label='Digit', ticks=range(10))\n",
    "plt.xlabel(f'PC1 ({pca_digits.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_digits.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "plt.title('Handwritten Digits: 64D ‚Üí 2D using PCA', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Some digits form distinct clusters (e.g., 0, 1, 6)\")\n",
    "print(\"- Other digits overlap (e.g., 3, 5, 8)\")\n",
    "print(\"- PCA captures enough structure to separate many classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction: How much information is lost?\n",
    "# Compare different numbers of components\n",
    "n_components_list = [2, 5, 10, 20, 30, 64]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(n_components_list), figsize=(18, 6))\n",
    "\n",
    "# Select one digit to reconstruct\n",
    "sample_idx = 0\n",
    "original_image = X_digits[sample_idx].reshape(8, 8)\n",
    "\n",
    "for i, n_comp in enumerate(n_components_list):\n",
    "    # Apply PCA with n_comp components\n",
    "    pca_temp = PCA(n_components=n_comp)\n",
    "    X_temp = pca_temp.fit_transform(X_digits_scaled)\n",
    "    X_reconstructed = pca_temp.inverse_transform(X_temp)\n",
    "    \n",
    "    # Inverse standardization\n",
    "    X_reconstructed = X_reconstructed * scaler_digits.scale_ + scaler_digits.mean_\n",
    "    reconstructed_image = X_reconstructed[sample_idx].reshape(8, 8)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    mse = np.mean((original_image - reconstructed_image) ** 2)\n",
    "    var_explained = np.sum(pca_temp.explained_variance_ratio_) * 100\n",
    "    \n",
    "    # Original\n",
    "    axes[0, i].imshow(original_image, cmap='gray')\n",
    "    axes[0, i].set_title('Original', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructed_image, cmap='gray')\n",
    "    axes[1, i].set_title(f'{n_comp} PCs\\n{var_explained:.1f}% var\\nMSE: {mse:.2f}', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Image Reconstruction with Different Numbers of Principal Components', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- With only 2 components: Digit is barely recognizable\")\n",
    "print(\"- With 10-20 components: Digit is clearly recognizable\")\n",
    "print(\"- With 64 components: Perfect reconstruction (no compression)\")\n",
    "print(\"\\nTrade-off: Compression vs. Information Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Limitations and Considerations\n",
    "\n",
    "### Limitations of PCA\n",
    "\n",
    "1. **Linearity**: PCA only captures linear relationships\n",
    "   - Solution: Use kernel PCA or other nonlinear methods (t-SNE, UMAP)\n",
    "\n",
    "2. **Variance ‚â† Information**: High variance doesn't always mean high importance\n",
    "   - Example: Outliers can create high-variance components that are just noise\n",
    "\n",
    "3. **Interpretability**: Principal components are linear combinations of all features\n",
    "   - Hard to interpret what each PC \"means\"\n",
    "\n",
    "4. **Scaling Sensitivity**: Results depend heavily on feature scaling\n",
    "   - Always standardize before PCA\n",
    "\n",
    "5. **Assumes Gaussian Distribution**: Works best when data is approximately normal\n",
    "\n",
    "### When NOT to Use PCA\n",
    "\n",
    "- When interpretability of original features is critical\n",
    "- When relationships are highly nonlinear\n",
    "- When you have very few features already\n",
    "- When features have very different meanings (e.g., mixing categorical and continuous)\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always standardize** your data before PCA\n",
    "2. **Check explained variance** to choose appropriate k\n",
    "3. **Visualize** the results (scree plot, biplots)\n",
    "4. **Consider alternatives** for nonlinear data (kernel PCA, t-SNE, UMAP)\n",
    "5. **Validate** on downstream tasks (classification, clustering, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Effect of scaling\n",
    "# Create data with features on different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.random.randn(200, 2)\n",
    "X_unscaled[:, 0] = X_unscaled[:, 0] * 100  # Feature 1: large scale\n",
    "X_unscaled[:, 1] = X_unscaled[:, 1] * 1    # Feature 2: small scale\n",
    "\n",
    "# PCA without scaling\n",
    "pca_unscaled = PCA(n_components=2)\n",
    "X_pca_unscaled = pca_unscaled.fit_transform(X_unscaled)\n",
    "\n",
    "# PCA with scaling\n",
    "scaler_demo = StandardScaler()\n",
    "X_scaled = scaler_demo.fit_transform(X_unscaled)\n",
    "pca_scaled = PCA(n_components=2)\n",
    "X_pca_scaled = pca_scaled.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Original unscaled data\n",
    "axes[0, 0].scatter(X_unscaled[:, 0], X_unscaled[:, 1], alpha=0.6, s=30)\n",
    "axes[0, 0].set_xlabel('Feature 1 (scale ~100)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Feature 2 (scale ~1)', fontsize=11)\n",
    "axes[0, 0].set_title('Original Data (Unscaled)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA without scaling\n",
    "axes[0, 1].scatter(X_pca_unscaled[:, 0], X_pca_unscaled[:, 1], alpha=0.6, s=30, color='red')\n",
    "axes[0, 1].set_xlabel('PC1', fontsize=11)\n",
    "axes[0, 1].set_ylabel('PC2', fontsize=11)\n",
    "axes[0, 1].set_title(f'PCA WITHOUT Scaling\\nPC1: {pca_unscaled.explained_variance_ratio_[0]*100:.1f}%, PC2: {pca_unscaled.explained_variance_ratio_[1]*100:.1f}%', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Original scaled data\n",
    "axes[1, 0].scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.6, s=30, color='green')\n",
    "axes[1, 0].set_xlabel('Feature 1 (standardized)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Feature 2 (standardized)', fontsize=11)\n",
    "axes[1, 0].set_title('Scaled Data (Standardized)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axis('equal')\n",
    "\n",
    "# PCA with scaling\n",
    "axes[1, 1].scatter(X_pca_scaled[:, 0], X_pca_scaled[:, 1], alpha=0.6, s=30, color='purple')\n",
    "axes[1, 1].set_xlabel('PC1', fontsize=11)\n",
    "axes[1, 1].set_ylabel('PC2', fontsize=11)\n",
    "axes[1, 1].set_title(f'PCA WITH Scaling\\nPC1: {pca_scaled.explained_variance_ratio_[0]*100:.1f}%, PC2: {pca_scaled.explained_variance_ratio_[1]*100:.1f}%', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEffect of Scaling on PCA:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nWITHOUT Scaling:\")\n",
    "print(f\"  PC1 explains: {pca_unscaled.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"  PC2 explains: {pca_unscaled.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(\"  ‚Üí PC1 dominated by high-variance Feature 1\")\n",
    "\n",
    "print(\"\\nWITH Scaling:\")\n",
    "print(f\"  PC1 explains: {pca_scaled.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"  PC2 explains: {pca_scaled.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(\"  ‚Üí Both features contribute more equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  Always standardize your data before PCA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Practice Exercises\n",
    "\n",
    "### Exercise 1: Wine Dataset\n",
    "\n",
    "Apply PCA to the wine dataset and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wine dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "print(\"Wine Dataset\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples: {X_wine.shape[0]}\")\n",
    "print(f\"Number of features: {X_wine.shape[1]}\")\n",
    "print(f\"Feature names: {wine.feature_names}\")\n",
    "print(f\"Target names: {wine.target_names}\")\n",
    "\n",
    "# TODO: Your task\n",
    "# 1. Standardize the data\n",
    "# 2. Apply PCA to reduce to 2 components\n",
    "# 3. Visualize the results colored by wine type\n",
    "# 4. Create a scree plot\n",
    "# 5. Interpret the results\n",
    "\n",
    "print(\"\\nüìù Your turn! Complete the exercise above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "print(\"Solution to Exercise 1\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Standardize\n",
    "scaler_wine = StandardScaler()\n",
    "X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
    "\n",
    "# 2. Apply PCA\n",
    "pca_wine = PCA(n_components=2)\n",
    "X_wine_pca = pca_wine.fit_transform(X_wine_scaled)\n",
    "\n",
    "print(f\"Variance explained by 2 components: {np.sum(pca_wine.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# 3. Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, (target, name) in enumerate(zip(range(3), wine.target_names)):\n",
    "    mask = y_wine == target\n",
    "    axes[0].scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1], \n",
    "                    c=colors[i], label=name, alpha=0.7, s=60, edgecolors='k', linewidth=0.5)\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[0].set_title('Wine Dataset: PCA Projection', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scree plot\n",
    "pca_wine_full = PCA()\n",
    "pca_wine_full.fit(X_wine_scaled)\n",
    "\n",
    "axes[1].bar(range(1, len(pca_wine_full.explained_variance_) + 1), \n",
    "            pca_wine_full.explained_variance_, alpha=0.7, color='steelblue')\n",
    "axes[1].plot(range(1, len(pca_wine_full.explained_variance_) + 1), \n",
    "             pca_wine_full.explained_variance_, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[1].set_ylabel('Eigenvalue', fontsize=12)\n",
    "axes[1].set_title('Scree Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- The three wine classes are well-separated in the 2D PCA space\")\n",
    "print(\"- First two components capture most of the variance\")\n",
    "print(\"- PCA successfully reduces 13D to 2D while preserving class structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create Your Own Dataset\n",
    "\n",
    "Create a synthetic 3D dataset and apply PCA to reduce it to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your task\n",
    "# 1. Create a 3D dataset with correlation between features\n",
    "# 2. Visualize in 3D\n",
    "# 3. Apply PCA to reduce to 2D\n",
    "# 4. Visualize in 2D\n",
    "# 5. Compare information loss\n",
    "\n",
    "print(\"üìù Your turn! Create and analyze your own dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **PCA is a linear dimensionality reduction technique** that finds directions of maximum variance\n",
    "\n",
    "2. **Mathematical foundation**:\n",
    "   - Based on eigendecomposition of covariance matrix\n",
    "   - Eigenvectors = principal components (directions)\n",
    "   - Eigenvalues = variance explained by each component\n",
    "\n",
    "3. **Algorithm steps**:\n",
    "   - Standardize data\n",
    "   - Compute covariance matrix\n",
    "   - Find eigenvalues and eigenvectors\n",
    "   - Project data onto top k eigenvectors\n",
    "\n",
    "4. **Choosing k components**:\n",
    "   - Use scree plot to find elbow\n",
    "   - Keep components until reaching variance threshold (e.g., 95%)\n",
    "   - Consider downstream task performance\n",
    "\n",
    "5. **Applications**:\n",
    "   - Visualization (2D/3D)\n",
    "   - Noise reduction\n",
    "   - Feature extraction\n",
    "   - Data compression\n",
    "   - Preprocessing for ML\n",
    "\n",
    "6. **Limitations**:\n",
    "   - Only captures linear relationships\n",
    "   - Sensitive to scaling\n",
    "   - Loss of interpretability\n",
    "   - Assumes variance = importance\n",
    "\n",
    "7. **Best practices**:\n",
    "   - Always standardize first\n",
    "   - Visualize explained variance\n",
    "   - Consider alternatives for nonlinear data\n",
    "   - Validate on downstream tasks\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **Kernel PCA**: Nonlinear extension using kernel trick\n",
    "- **t-SNE**: Nonlinear dimensionality reduction for visualization\n",
    "- **UMAP**: Modern alternative to t-SNE\n",
    "- **Autoencoders**: Neural network-based dimensionality reduction\n",
    "- **Factor Analysis**: Probabilistic alternative to PCA\n",
    "\n",
    "### Practice Recommendations\n",
    "\n",
    "1. Apply PCA to real datasets (Kaggle, UCI ML Repository)\n",
    "2. Compare PCA with other dimensionality reduction methods\n",
    "3. Use PCA as preprocessing for classification/regression\n",
    "4. Experiment with different numbers of components\n",
    "5. Visualize high-dimensional data using PCA\n",
    "\n",
    "---\n",
    "\n",
    "**End of Tutorial**\n",
    "\n",
    "*Prepared for DA5400W - Foundations of Machine Learning*  \n",
    "*Dr. Arun B Ayyar, IIT Madras*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
